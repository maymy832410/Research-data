
@article{noauthor_skill_2005,
	title = {Skill shortage in engineering prompts {Government} response},
	volume = {47},
	issn = {0040-0912},
	url = {https://doi.org/10.1108/et.2005.00447dab.006},
	doi = {10.1108/et.2005.00447dab.006},
	number = {4/5},
	urldate = {2023-11-10},
	journal = {Education + Training},
	month = jan,
	year = {2005},
	note = {Publisher: Emerald Group Publishing Limited},
}

@article{lund_prompt_2023,
	title = {The prompt engineering librarian},
	volume = {40},
	issn = {0741-9058},
	url = {https://doi.org/10.1108/LHTN-10-2023-0189},
	doi = {10.1108/LHTN-10-2023-0189},
	abstract = {Purpose In terms of training the public in prompt engineering skills, no single discipline or profession currently takes the lead, presenting an opportunity for professions like librarianship to step into this role. Librarians are already well-equipped to educate the public in a wide range of literacy skills and tasks, so prompt engineering may be a natural progression. The purpose of this paper is to examine the potential role of prompt engineering for library professionals. Design/methodology/approach Prompt engineering is the process of optimizing the text that is provided to an artificial intelligence (A)I model to ensure proper interpretation and the generation of relevant, detailed results. The field of prompt engineering is relatively young, evolving alongside the growth of large language models like ChatGPT and BARD. This conceptual paper will explore prompt engineering as a possible domain of expertise for librarians. Findings This paper delves into the world of prompt engineering, its alignment with the existing roles and expertise of librarians, and the potential emergence of a new role known as the “prompt engineering librarian,” akin to the well-established “information literacy librarian” role that has gained prominence in recent decades. Originality/value The significance of this work lies in exploring the synergy between prompt engineering and the traditional roles of librarians, highlighting the potential for a new and valuable profession in the form of prompt engineering librarians. This innovative concept could bridge the gap in AI literacy and facilitate more effective interactions with AI systems, contributing to the broader goal of AI accessibility and understanding.},
	number = {8},
	urldate = {2023-11-10},
	journal = {Library Hi Tech News},
	author = {Lund, Brady},
	month = jan,
	year = {2023},
	note = {Publisher: Emerald Publishing Limited},
	pages = {6--8},
	annote = {Cited by: 0},
}

@article{cheung_real_2023,
	title = {Real {Estate} {Insights} {Unleashing} the potential of {ChatGPT} in property valuation reports: the “{Red} {Book}” compliance {Chain}-of-thought ({CoT}) prompt engineering},
	volume = {ahead-of-print},
	issn = {1463-578X},
	url = {https://doi.org/10.1108/JPIF-06-2023-0053},
	doi = {10.1108/JPIF-06-2023-0053},
	abstract = {Purpose This viewpoint article explores the transformative capabilities of large language models (LLMs) like the Chat Generative Pre-training Transformer (ChatGPT) within the property valuation industry. It particularly accentuates the pivotal role of prompt engineering in facilitating valuation reporting and advocates for adopting the “Red Book” compliance Chain-of-thought (COT) prompt engineering as a gold standard for generating AI-facilitated valuation reports. Design/methodology/approach The article offers a high-level examination of the application of LLMs in real estate research, highlighting the essential role of prompt engineering for future advancements in generative AI. It explores the collaborative dynamic between valuers and AI advancements, emphasising the importance of precise instructions and contextual cues in directing LLMs to generate accurate and reproducible valuation outcomes. Findings Integrating LLMs into property valuation processes paves the way for efficiency improvements and task automation, such as generating reports and drafting contracts. AI-facilitated reports offer unprecedented transparency and elevate client experiences. The fusion of valuer expertise with prompt engineering ensures the reliability and interpretability of valuation reports. Practical implications Delineating the types and versions of LLMs used in AI-generated valuation reports encourage the adoption of transparency best practices within the industry. Valuers, as expert prompt engineers, can harness the potential of AI to enhance efficiency, accuracy and transparency in the valuation process, delivering significant benefits to a broad array of stakeholders. Originality/value The article elucidates the substantial impact of prompt engineering in leveraging LLMs within the property industry. It underscores the importance of valuers training their unique GPT models, enabling customisation and reproducibility of valuation outputs. The symbiotic relationship between valuers and LLMs is identified as a key driver shaping the future of property valuations.},
	number = {ahead-of-print},
	urldate = {2023-11-10},
	journal = {Journal of Property Investment \& Finance},
	author = {Cheung, Ka Shing},
	month = jan,
	year = {2023},
	note = {Publisher: Emerald Publishing Limited},
	annote = {Cited by: 2},
}

@inproceedings{wu_cheap-fake_2023,
	title = {Cheap-{Fake} {Detection} with {LLM} {Using} {Prompt} {Engineering}},
	doi = {10.1109/ICMEW59549.2023.00025},
	abstract = {The misuse of real photographs with conflicting image captions in news items is an example of the out-of-context (OOC) misuse of media. In order to detect OOC media, individuals must determine the accuracy of the statement and evaluate whether the triplet (i.e., the image and two captions) relates to the same event. This paper presents a novel learnable approach for detecting OOC media in ICME'23 Grand Challenge on Detecting Cheapfakes. The proposed method is based on the COSMOS structure, which assesses the coherence between an image and captions, as well as between two captions. We enhance the baseline algorithm by incorporating a Large Language Model (LLM), GPT3.5, as a feature extractor. Specifically, we propose an innovative approach to feature extraction utilizing prompt engineering to develop a robust and reliable feature extractor with GPT3.5 model. The proposed method captures the correlation between two captions and effectively integrates this module into the COSMOS baseline model, which allows for a deeper understanding of the relationship between captions. By incorporating this module, we demonstrate the potential for significant improvements in cheap-fakes detection performance. The proposed methodology holds promising implications for various applications such as natural language processing, image captioning, and text-to-image synthesis. Docker for submission is available at https://hub.docker.com/repository/docker/mulns/acmmmcheapfakes.},
	booktitle = {2023 {IEEE} {International} {Conference} on {Multimedia} and {Expo} {Workshops} ({ICMEW})},
	author = {Wu, Guangyang and Wu, Weijie and Liu, Xiaohong and Xu, Kele and Wan, Tianjiao and Wang, Wenyi},
	month = jul,
	year = {2023},
	keywords = {Prompt engineering, Large language model, Language model, Image caption, Computational linguistics, Cheap-fake, Fake detection, Feature extractor, Features extraction, Grand Challenge, Innovative approaches, Natural language processing systems},
	pages = {105--109},
	annote = {Cited by: 0; Conference name: 2023 IEEE International Conference on Multimedia and Expo Workshops, ICMEW 2023; Conference date: 10 July 2023 through 14 July 2023; Conference code: 192081; All Open Access, Green Open Access},
}

@inproceedings{spasic_using_2023,
	title = {Using {ChatGPT} {Standard} {Prompt} {Engineering} {Techniques} in {Lesson} {Preparation}: {Role}, {Instructions} and {Seed}-{Word} {Prompts}},
	doi = {10.1109/ICEST58410.2023.10187269},
	abstract = {The application of available natural language processing systems can have a significant impact on the education process. The primary aim of this research was to test the impact of three standard prompting techniques on the results obtained from ChatGPT. Generation of a lesson plan for programming for preschoolers was chosen as the task set for AI. The obtained results show that use of a standard prompting with additional defined roles and seed words can be useful in preparation of teaching units and lessons and it can be considered as a technique of teachers' choice.},
	booktitle = {2023 58th {International} {Scientific} {Conference} on {Information}, {Communication} and {Energy} {Systems} and {Technologies} ({ICEST})},
	author = {Spasić, Aleksandar J. and Janković, Dragan S.},
	month = jun,
	year = {2023},
	keywords = {ChatGPT, Prompt engineering, Natural language processing systems, Teaching, Education process, Engineering techniques, IT education lesson plan, IT-education, Lesson plans, Prompt technique, Seed words, Teachers'},
	pages = {47--50},
	annote = {Cited by: 0; Conference name: 58th International Scientific Conference on Information, Communication and Energy Systems and Technologies, ICEST 2023; Conference date: 29 June 2023 through 1 July 2023; Conference code: 191047},
}

@inproceedings{samanta_insightssumm_2023,
	title = {{InsightsSumm} - {Summarization} of {ITOps} {Incidents} {Through} {In}-{Context} {Prompt} {Engineering}},
	doi = {10.1109/CLOUD60044.2023.00041},
	abstract = {AI has been extensively used to help Site Reliability Engineers (SREs) to resolve faults in cloud services and applications. It helps to accelerate resolution time by navigating through the vast amount of heterogeneous data (logs, metrics, alerts, etc) related to a fault. A good ITOps system should help SREs by giving precise and meaningful insights for a quick understanding of the data at hand. In this paper, we design a framework to summarize the context or insight present in the heterogeneous data related to a fault. The proposed framework constructs queries/prompts, specific to the ITOps domain, which helps us to generate more insightful abstractive summaries using state-of-the-art text generator models. Initial study on simulated faults shows promising results, which can be expanded to accommodate other datatype, providing summaries for real-world cases.},
	booktitle = {2023 {IEEE} 16th {International} {Conference} on {Cloud} {Computing} ({CLOUD})},
	author = {Samanta, Suranjana and Chatterjee, Oishik and Boyette, Neil and Liu, Guangya and Mohapatra, Prateeti},
	month = jul,
	year = {2023},
	note = {ISSN: 2159-6190},
	keywords = {Prompt engineering, Abstractive summary, Cloud applications, Context learning, Heterogeneous data, In contexts, In-context learning, Incident insight, Metric evaluation, Query processing, Reliability engineers},
	pages = {290--292},
	annote = {Cited by: 0; Conference name: 16th IEEE International Conference on Cloud Computing, CLOUD 2023; Conference date: 2 July 2023 through 8 July 2023; Conference code: 192947},
}

@inproceedings{ali_prompt_2023,
	title = {Prompt {Engineering} in {Medical} {Image} {Segmentation}: {An} {Overview} of the {Paradigm} {Shift}},
	doi = {10.1109/AIBThings58340.2023.10292475},
	abstract = {Foundation AI models have emerged as powerful pre-trained models on a large scale, capable of seamlessly handling diverse tasks across multiple domains with minimal or no fine-tuning. These models, exemplified by the impressive achievements of GPT-3 and BERT in natural language processing (NLP), as well as CLIP and DALL-E in computer vision, have garnered considerable attention for their exceptional performance. A noteworthy addition to the realm of image segmentation is the Segment Anything Model (SAM), a foundation AI model that revolutionizes image segmentation. With a single click or a natural language prompt, SAM exhibits the remarkable ability to segment any object within an image, marking a significant paradigm shift in medical image segmentation. Unlike conventional approaches that rely on labeled data and domain-specific knowledge, SAM breaks free from these constraints. Deep convolutional neural network (DCNN)-based, SAM comprises an image encoder, a prompt encoder, and a mask decoder, showcasing its efficient and flexible architecture. Medical image segmentation, in particular, benefits from SAM’s exceptional speed and high-quality segmentation. In this paper, we delve into the effectiveness of SAM for medical image segmentation shedding light on its capabilities. Moreover, our investigation explores the strengths and limitations of prompt engineering in medical computer vision applications, not only encompassing SAM but also other foundation AI models. Through this exploration, we unravel their immense potential in catalyzing a paradigm shift in the field of medical imaging.},
	booktitle = {2023 {IEEE} {International} {Conference} on {Artificial} {Intelligence}, {Blockchain}, and {Internet} of {Things} ({AIBThings})},
	author = {Ali, Hazrat and Bulbul, Mohammad Farhad and Shah, Zubair},
	month = sep,
	year = {2023},
	pages = {1--4},
}

@inproceedings{rodriguez_prompts_2023,
	title = {Prompts {Matter}: {Insights} and {Strategies} for {Prompt} {Engineering} in {Automated} {Software} {Traceability}},
	doi = {10.1109/REW57809.2023.00087},
	abstract = {Large Language Models (LLMs) have the potential to revolutionize automated traceability by overcoming the challenges faced by previous methods and introducing new possibilities. However, the optimal utilization of LLMs for automated traceability remains unclear. This paper explores the process of prompt engineering to extract link predictions from an LLM. We provide detailed insights into our approach for constructing effective prompts, offering our lessons learned. Additionally, we propose multiple strategies for leveraging LLMs to generate traceability links, improving upon previous zero-shot methods on the ranking of candidate links after prompt refinement. The primary objective of this paper is to inspire and assist future researchers and engineers by highlighting the process of constructing traceability prompts to effectively harness LLMs for advancing automatic traceability.},
	booktitle = {2023 {IEEE} 31st {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Rodriguez, Alberto D. and Dearstyne, Katherine R. and Cleland-Huang, Jane},
	month = sep,
	year = {2023},
	note = {ISSN: 2770-6834},
	keywords = {Prompt engineering, Large language model, Language model, Zero-shot learning, Computational linguistics, Automated software traceability, Automation, Link prediction, Multiple strategy, Optimal utilization, Primary objective, Software traceability, Traceability links},
	pages = {455--464},
	annote = {Cited by: 0; Conference name: 31st IEEE International Requirements Engineering Conference Workshops, REW 2023; Conference date: 4 September 2023 through 8 September 2023; Conference code: 193031; All Open Access, Green Open Access},
}

@article{wang_unleashing_2023,
	title = {Unleashing {ChatGPT}'s {Power}: {A} {Case} {Study} on {Optimizing} {Information} {Retrieval} in {Flipped} {Classrooms} via {Prompt} {Engineering}},
	issn = {1939-1382},
	doi = {10.1109/TLT.2023.3324714},
	abstract = {This research project investigates the impact of Prompt Engineering, a key aspect of ChatGPT, on college students' information retrieval in flipped classrooms. In recent years, an increasing number of students have been using AI-based tools such as ChatGPT rather than traditional research engines to learn and to complete course assignments. Despite this growing trend, previous research has largely overlooked the influence of prompt engineering on students' use of ChatGPT and effective strategies for improving the quality of information retrieval in learning settings. To address this research gap, this study examines the information quality obtained from ChatGPT in a flipped classroom by evaluating its effectiveness in task completion among 26 novice undergraduates from the same major and cohort. The experimental results provide evidence that proficient mastery of prompt engineering improves the quality of information obtained by students using ChatGPT. Consequently, by acquiring proficiency in prompt engineering, students can maximize the positive impact of ChatGPT, obtain high-quality information, and enhance their learning efficiency in flipped classrooms.},
	journal = {IEEE Transactions on Learning Technologies},
	author = {Wang, Mo and Wang, Minjuan and Xu, Xin and Yang, Lanqing and Cai, Dunbo and Yin, Minghao},
	year = {2023},
	keywords = {ChatGPT, Prompt engineering, Transformer, Engineering education, Information retrieval, Students, Power, E-learning, Chatbots, Electronic learning, Flipped classroom, Job analysis, On-line service, Oral communication, Task analysis, Teaching},
	pages = {1--13},
	annote = {Cited by: 0},
}

@article{strobelt_interactive_2023,
	title = {Interactive and {Visual} {Prompt} {Engineering} for {Ad}-hoc {Task} {Adaptation} with {Large} {Language} {Models}},
	volume = {29},
	issn = {1941-0506},
	doi = {10.1109/TVCG.2022.3209479},
	abstract = {State-of-the-art neural language models can now be used to solve ad-hoc language tasks through zero-shot prompting without the need for supervised training. This approach has gained popularity in recent years, and researchers have demonstrated prompts that achieve strong accuracy on specific NLP tasks. However, finding a prompt for new tasks requires experimentation. Different prompt templates with different wording choices lead to significant accuracy differences. PromptIDE allows users to experiment with prompt variations, visualize prompt performance, and iteratively optimize prompts. We developed a workflow that allows users to first focus on model feedback using small data before moving on to a large data regime that allows empirical grounding of promising prompts using quantitative measures of the task. The tool then allows easy deployment of the newly created ad-hoc models. We demonstrate the utility of PromptIDE (demo: http://prompt.vizhub.ai) and our workflow using several real-world use cases.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Strobelt, Hendrik and Webson, Albert and Sanh, Victor and Hoover, Benjamin and Beyer, Johanna and Pfister, Hanspeter and Rush, Alexander M.},
	month = jan,
	year = {2023},
	keywords = {Natural language processing, Transformer, Language model, Visual languages, Zero-shot learning, Computational linguistics, Natural language processing systems, Language processing, Natural languages, article, human, language, Job analysis, Task analysis, Computational modelling, human experiment, Modeling languages, quantitative analysis, workflow, Zero-shot model},
	pages = {1146--1156},
	annote = {Cited by: 8; All Open Access, Green Open Access},
}

@inproceedings{pehlivanoglu_enhancing_2023,
	title = {Enhancing {Paraphrasing} in {Chatbots} {Through} {Prompt} {Engineering}: {A} {Comparative} {Study} on {ChatGPT}, {Bing}, and {Bard}},
	doi = {10.1109/UBMK59864.2023.10286606},
	abstract = {Paraphrase generation, a crucial task in Natural Language Processing (NLP), is pivotal for the effectiveness of AI chatbots. However, generating high-quality paraphrases that are contextually relevant, semantically equivalent, and linguistically diverse remains a challenge. This paper explores the use of prompt engineering to enhance the paraphrasing capabilities of AI chatbots, specifically focusing on ChatGPT, Bing, and Bard. We introduce a new dataset of 5000 sentences generated by ChatGPT across diverse topics and propose two distinct prompts for paraphrase generation: a direct approach and an engineered prompt. The engineered prompt explicitly instructs the chatbot to generate paraphrases that exhibit lexical diversity, phrasal variations, syntactical differences, fluency, language acceptableness, and relevance, while preserving the original meaning. We conduct a comprehensive evaluation of the generated paraphrases using a range of metrics, including BERTScore, STS-B, METEOR for semantic similarity; ROUGE, BLEU, GLEU for diversity; and CoLA, Perplexity for language acceptableness or fluency. Our findings reveal that the use of the engineered prompt results in higher quality paraphrases across all three chatbots, demonstrating the potential of prompt engineering as a tool for improving chatbot communication.},
	booktitle = {2023 8th {International} {Conference} on {Computer} {Science} and {Engineering} ({UBMK})},
	author = {Pehlivanoğlu, Meltem Kurt and Syakura, Muhammad Abdan and Duru, Nevcihan},
	month = sep,
	year = {2023},
	note = {ISSN: 2521-1641},
	pages = {432--437},
}

@article{feng_promptmagician_2023,
	title = {{PromptMagician}: {Interactive} {Prompt} {Engineering} for {Text}-to-{Image} {Creation}},
	issn = {1941-0506},
	doi = {10.1109/TVCG.2023.3327168},
	abstract = {Generative text-to-image models have gained great popularity among the public for their powerful capability to generate high-quality images based on natural language prompts. However, developing effective prompts for desired images can be challenging due to the complexity and ambiguity of natural language. This research proposes PromptMagician, a visual analysis system that helps users explore the image results and refine the input prompts. The backbone of our system is a prompt recommendation model that takes user prompts as input, retrieves similar prompt-image pairs from DiffusionDB, and identifies special (important and relevant) prompt keywords. To facilitate interactive prompt refinement, PromptMagician introduces a multi-level visualization for the cross-modal embedding of the retrieved images and recommended keywords, and supports users in specifying multiple criteria for personalized exploration. Two usage scenarios, a user study, and expert interviews demonstrate the effectiveness and usability of our system, suggesting it facilitates prompt engineering and improves the creativity support of the generative text-to-image model.},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Feng, Yingchaojie and Wang, Xingbo and Wong, Kam Kwai and Wang, Sijia and Lu, Yuhong and Zhu, Minfeng and Wang, Baicheng and Chen, Wei},
	year = {2023},
	pages = {1--11},
}

@inproceedings{abukhalaf_codex_2023,
	title = {On {Codex} {Prompt} {Engineering} for {OCL} {Generation}: {An} {Empirical} {Study}},
	doi = {10.1109/MSR59073.2023.00033},
	abstract = {The Object Constraint Language (OCL) is a declarative language that adds constraints and object query expressions to Meta-Object Facility (MOF) models. OCL can provide precision and conciseness to UML models. Nevertheless, the unfamiliar syntax of OCL has hindered its adoption by software practitioners. LLMs, such as GPT-3, have made significant progress in many NLP tasks, such as text generation and semantic parsing. Similarly, researchers have improved on the downstream tasks by fine-tuning LLMs for the target task. Codex, a GPT-3 descendant by OpenAI, has been fine-tuned on publicly available code from GitHub and has proven the ability to generate code in many programming languages, powering the AI-pair programmer Copilot. One way to take advantage of Codex is to engineer prompts for the target downstream task. In this paper, we investigate the reliability of the OCL constraints generated by Codex from natural language specifications. To achieve this, we compiled a dataset of 15 UML models and 168 specifications from various educational resources. We manually crafted a prompt template with slots to populate with the UML information and the target task in the prefix format to complete the template with the generated OCL constraint. We used both zero- and few-shot learning methods in the experiments. The evaluation is reported by measuring the syntactic validity and the execution accuracy metrics of the generated OCL constraints. Moreover, to get insight into how close or natural the generated OCL constraints are compared to human-written ones, we measured the cosine similarity between the sentence embedding of the correctly generated and human-written OCL constraints. Our findings suggest that by enriching the prompts with the UML information of the models and enabling few-shot learning, the reliability of the generated OCL constraints increases. Furthermore, the results reveal a close similarity based on sentence embedding between the generated OCL constraints and the human-written ones in the ground truth, implying a level of clarity and understandability in the generated OCL constraints by Codex.},
	booktitle = {2023 {IEEE}/{ACM} 20th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Abukhalaf, Seif and Hamdaqa, Mohammad and Khomh, Foutse},
	month = may,
	year = {2023},
	note = {ISSN: 2574-3864},
	keywords = {Prompt engineering, Large language model, Language model, Natural language processing systems, Codegeneration, Codes (symbols), Codex, Down-stream, Embeddings, Object constraint language, Semantics, Specifications, Syntactics, UML Model, Unified Modeling Language},
	pages = {148--157},
	annote = {Cited by: 0; Conference name: 20th IEEE/ACM International Conference on Mining Software Repositories, MSR 2023; Conference date: 15 May 2023 through 16 May 2023; Conference code: 190670; All Open Access, Green Open Access},
}

@inproceedings{gorer_generating_2023,
	title = {Generating {Requirements} {Elicitation} {Interview} {Scripts} with {Large} {Language} {Models}},
	doi = {10.1109/REW57809.2023.00015},
	abstract = {Requirements elicitation interviews are the most popular requirements elicitation technique and an integral part of requirements engineering education. Good and bad interview scripts provide students with examples of applying the theory. Constructing an interview script requires technical knowledge, practical experience, and creativity. As a result, only a few educational interview scripts are available to the community. This paper explores automatically generating interview scripts with large language models through prompt engineering. Our contribution is two-fold: First, we present a graph representation of interactive interview scripts. Second, we apply prompt engineering techniques to generate business domain descriptions, linear scripts, and conversation pieces focused on certain types of mistakes. Our findings indicate that large language models face challenges in handling interview conversation graphs. However, we can enhance the quality of the generated interview scripts by decomposing the task into smaller components and refining the prompts to provide more precise instructions.},
	booktitle = {2023 {IEEE} 31st {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Görer, Binnur and Aydemir, Fatma Başak},
	month = sep,
	year = {2023},
	note = {ISSN: 2770-6834},
	pages = {44--51},
}

@inproceedings{gamage_augmenting_2023,
	title = {Augmenting {Industrial} {Chatbots} in {Energy} {Systems} using {ChatGPT} {Generative} {AI}},
	doi = {10.1109/ISIE51358.2023.10228101},
	abstract = {Chatbots, the automation of communicative labor, have been widely deployed in industrial applications and systems. Built upon the Generative Pre-trained Transformer 3 (GPT-3), ChatGPT is a Generative Artificial Intelligence (AI) primed to transform all pre-existing chatbot capabilities with human-like conversation skills. It has already disrupted many disciplines including tertiary education and academic research methods, with increasing adoption in simple to complex tasks. However, the augmentation of pre-existing industrial chatbots with generative AI capabilities has not been fully investigated and demonstrated in recent literature. In this paper, we address this gap by presenting the augmentation of a pre-existing chatbot using ChatGPT generative AI capabilities. Our contribution encompasses the ten primary human-like conversation capabilities of ChatGPT, its augmentation of the pre-existing functionalities and the adopted prompt engineering strategies. Each capability is empirically demonstrated on Cooee, a functionally deployed chatbot in the microgrid energy systems of the La Trobe Energy Analytics Platform (LEAP).},
	booktitle = {2023 {IEEE} 32nd {International} {Symposium} on {Industrial} {Electronics} ({ISIE})},
	author = {Gamage, Gihan and Kahawala, Sachin and Mills, Nishan and De Silva, Daswin and Manic, Milos and Alahakoon, Damminda and Jennings, Andrew},
	month = jun,
	year = {2023},
	note = {ISSN: 2163-5145},
	pages = {1--6},
}

@inproceedings{jain_transformer-based_2023,
	title = {A {Transformer}-based {Approach} for {Abstractive} {Summarization} of {Requirements} from {Obligations} in {Software} {Engineering} {Contracts}},
	doi = {10.1109/RE57278.2023.00025},
	abstract = {Software Engineering (SE) contracts are a valuable source of software requirements. Seed requirements derived from SE contracts can provide a starting point to the Requirements Engineering (RE) phase. To extract such a seed however, a correct interpretation of contracts text is crucial. A major challenge with contracts text interpretation is that the text is lengthy, convoluted, and it incorporates a complex Legalese. If a summary of the high-level requirements from obligations present in SE contracts is available to the requirement analysts in a language that is comprehensible to them, they can use this seed requirements knowledge to ask the right questions to the stakeholders. In this paper, we propose an approach for summarizing the requirements present in obligations in a language comprehensible to requirement analysts. We use the principles of Prompt Engineering to prompt GPT-3 to generate summaries for training Natural Language Generation (NLG) models for generating SE-specific summaries. Experiments using NLG models such as BART, GPT-2, T5, and Pegasus indicate that Pegasus generates the most accurate summaries with the highest ROUGE score as compared to other models.},
	booktitle = {2023 {IEEE} 31st {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Jain, Chirag and Anish, Preethu Rose and Singh, Amrita and Ghaisas, Smita},
	month = sep,
	year = {2023},
	note = {ISSN: 2332-6441},
	pages = {169--179},
}

@inproceedings{das_zero-shot_2023,
	title = {Zero-shot {Learning} for {Named} {Entity} {Recognition} in {Software} {Specification} {Documents}},
	doi = {10.1109/RE57278.2023.00019},
	abstract = {Named entity recognition (NER) is a natural language processing task that has been used in Requirements Engineering for the identification of entities such as actors, actions, operators, resources, events, GUI elements, hardware, APIs, and others. NER might be particularly useful for extracting key information from Software Requirements Specification documents, which provide a blueprint for software development. However, a common challenge in this domain is the lack of annotated data. In this article, we propose and analyze two zero-shot approaches for NER in the requirements engineering domain. These are found to be particularly effective in situations where labeled data is scarce or non-existent. The first approach is a template-based zero-shot learning mechanism that uses the prompt engineering approach and achieves 93\% accuracy according to our experimental results. The second solution takes an orthogonal approach by transforming the entity recognition problem into a question-answering task which results in 98\% accuracy. Both zero-shot NER approaches introduced in this work perform better than the existing state-of-the-art solutions in the requirements engineering domain.},
	booktitle = {2023 {IEEE} 31st {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Das, Souvick and Deb, Novarun and Cortesi, Agostino and Chaki, Nabendu},
	month = sep,
	year = {2023},
	note = {ISSN: 2332-6441},
	pages = {100--110},
}

@inproceedings{bakr_specialized_2023,
	title = {Specialized {Syntactic} {Quran} {Search} {Engines}: {Evaluation} and {Limitations}},
	doi = {10.1109/IMSA58542.2023.10217550},
	abstract = {The Quran is the sacred text that provides guidance and teachings to the followers of Islam. This paper aims to analyze and evaluate the limitations of current specialized search engines used for retrieving information from the Quran. Also, this work includes an initial evaluation of Quran search with a large language model (LLM) employing prompt engineering. The study focuses on the syntactic aspect of information retrieval, while acknowledging the necessity of considering the semantic meaning of Quranic words and verses for a more comprehensive analysis. Furthermore, recommendations and guidelines for future research are proposed, stressing the significance of developing syntactic search capabilities to improve the accuracy and relevance of search results.},
	booktitle = {2023 {Intelligent} {Methods}, {Systems}, and {Applications} ({IMSA})},
	author = {Bakr, Abdollah and Yousef, Ahmed H. and Arafa, Tamer},
	month = jul,
	year = {2023},
	pages = {269--275},
}

@inproceedings{lei_using_2023,
	title = {Using {ChatGPT} on {Improving} {Program} {Performance} with pprof and {Benchmark}},
	doi = {10.1109/ICCCI59363.2023.10210148},
	abstract = {In the context of limited computing resources, optimizing program architecture is crucial. Therefore, this paper proposes to apply the powerful analytical capabilities of large language models (LLM) to the field of systematic performance optimization. The output of pprof and benchmark is fed into ChatGPT, and the program's performance is improved based on feedback. In the case study, the number of memory allocations for the objective function was successfully reduced from 99 to 1, resulting in a reduction of the test execution time from 8.6 microseconds to 0.36 microseconds. At the same time, the memory allocation was also reduced from 53.5KB to approximately 1KB.},
	booktitle = {2023 5th {International} {Conference} on {Computer} {Communication} and the {Internet} ({ICCCI})},
	author = {Lei, Wei-Cheng and Jian, Luo-You and Chen, Yan-Wen and Chou, Li-Der},
	month = jun,
	year = {2023},
	note = {ISSN: 2833-2350},
	pages = {256--260},
}

@inproceedings{liu_sam-based_2023,
	title = {A {SAM}-based method for large-scale crop field boundary delineation},
	doi = {10.1109/SECON58729.2023.10287502},
	abstract = {Large-scale digitalization of crop field boundaries provides vital information for smart agriculture applications. Due to the high cost of high-resolution remote sensing images and the time-consuming manual annotation of data, effective and budget-friendly solutions for extracting closed agricultural field boundaries remain scarce. Recently, the emergence of the foundation model, Segment Anything Model (SAM), has had a profound impact on the field of computer vision and prompted efforts to migrate it to particular fields. In order to explore the potential of the SAM for agricultural land segmentation, we propose a workflow that automatically instructs the SAM model for large-scale farmland division by extracting spatialtemporal features from remote sensing images as auxiliary information. The methodology was evaluated on an experimental area larger than 1,000 square kilometres, and the preliminary results corroborate its applicability and viability.},
	booktitle = {2023 20th {Annual} {IEEE} {International} {Conference} on {Sensing}, {Communication}, and {Networking} ({SECON})},
	author = {Liu, Xuanyu},
	month = sep,
	year = {2023},
	note = {ISSN: 2155-5494},
	pages = {1--6},
}

@inproceedings{phokela_smart_2023,
	title = {Smart {Prompt} {Advisor}: {Multi}-{Objective} {Prompt} {Framework} for {Consistency} and {Best} {Practices}},
	doi = {10.1109/ASE56229.2023.00019},
	abstract = {Recent breakthroughs in Large Language Models (LLM), comprised of billions of parameters, have achieved the ability to unveil exceptional insight into a wide range of Natural Language Processing (NLP) tasks. The onus of the performance of these models lies in the sophistication and completeness of the input prompt. Minimizing the enhancement cycles of prompt with improvised keywords becomes critically important as it directly affects the time to market and cost of the developing solution. However, this process inevitably has a trade-off between the learning curve/proficiency of the user and completeness of the prompt, as generating such a solutions is an incremental process. In this paper, we have designed a novel solution and implemented it in the form of a plugin for Visual Studio Code IDE, which can optimize this trade-off, by learning the underlying prompt intent to enhance with keywords. This will tend to align with developers' collection of semantics while developing a secure code, ensuring parameter and local variable names, return expressions, simple pre and post-conditions. and basic control and data flow are met.},
	booktitle = {2023 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Phokela, Kanchanjot Kaur and Sikand, Samarth and Singi, Kapil and Dey, Kuntal and Sharma, Vibhu Saujanya and Kaulgud, Vikrant},
	month = sep,
	year = {2023},
	note = {ISSN: 2643-1572},
	pages = {1846--1848},
}

@inproceedings{sun_prompt_2023,
	title = {Prompt {Learning} {Under} the {Large} {Language} {Model}},
	doi = {10.1109/SCSET58950.2023.00070},
	abstract = {With the emergence of models such as chatGPT and Baidu AI Wenxin Yiyan, the research and application of NLP (Natural Language Processing) is increasingly centered on PLM (Pretrained Language Model),It marks that the current machine learning model has reached a new height. This article first introduces the background of the large language model, and introduces pre-training + fine-tuning and the current popular Prompt from the four major paradigms of NLP. Understand the workflow and function of Prompt, focusing on Prompt engineering, and structures, and looking ahead to future challenges for Prompt.},
	booktitle = {2023 {International} {Seminar} on {Computer} {Science} and {Engineering} {Technology} ({SCSET})},
	author = {Sun, Lili and Shi, Zhenquan},
	month = apr,
	year = {2023},
	pages = {288--291},
}

@inproceedings{ezenkwu_towards_2023,
	title = {Towards {Expert} {Systems} for {Improved} {Customer} {Services} {Using} {ChatGPT} as an {Inference} {Engine}},
	doi = {10.1109/ICDATE58146.2023.10248647},
	abstract = {By harnessing both implicit and explicit customer data, companies can develop a more comprehensive understanding of their consumers, leading to better customer engagement and experience, and improved loyalty. As a result, businesses have embraced many AI technologies, including chatbots, sentiment analysis, voice assistants, predictive analytics, and natural language processing, within customer services and e-commerce. The arrival of ChatGPT, a state-of-the-art deep learning model trained with general knowledge in mind, has brought about a paradigm shift in how companies approach AI applications. However, given that most business problems are bespoke and require specialised domain expertise, ChatGPT needs to be aligned with the requisite task-oriented ability to solve these issues. This paper presents an iterative procedure that incorporates expert system development process models and prompt engineering, in the design of descriptive knowledge and few-shot prompts, as are necessary for ChatGPT-powered expert systems applications within customer services. Furthermore, this paper explores potential application areas for ChatGPT-powered expert systems in customer services, presenting opportunities for their effective utilisation in the business sector.},
	booktitle = {2023 {International} {Conference} on {Digital} {Applications}, {Transformation} \& {Economy} ({ICDATE})},
	author = {Ezenkwu, Chinedu Pascal},
	month = jul,
	year = {2023},
	pages = {1--5},
}

@inproceedings{cui_be-or-not_2023,
	title = {Be-or-{Not} {Prompt} {Enhanced} {Hard} {Negatives} {Generating} {For} {Memes} {Category} {Detection}},
	doi = {10.1109/ICME55011.2023.00038},
	abstract = {Memes are one of the most popular social media in online disinformation campaigns. Their creators often use a variety of rhetoric and psychological skills to achieve the purpose of misinformed audiences. These characteristics lead to the unsatisfactory performance of memes category detection tasks, such as predicting propaganda techniques, being harmful or not, and so on. To this end, we propose a novel memes category detection model via Be-or-Not Prompt Enhanced hard Negatives generating (BNPEN). Firstly, our BNPEN is reformulated into a contrastive learning-based image-text matching (ITM) task through category-padded prompt engineering. Secondly, we design the be-or-not prompt templates to keep the writing style of memes and create hard negative image-text pairs. Finally, our negatives generating can alleviate the negative-positive-coupling (NPC) effects in contrastive learning, thus improving the image-text matching quality. Conducted on two public datasets, experimental results show that our BNPEN is better than the off-the-shelf multi-modal learning models in terms of F1 and Accuracy measures.},
	booktitle = {2023 {IEEE} {International} {Conference} on {Multimedia} and {Expo} ({ICME})},
	author = {Cui, Jian and Li, Lin and Tao, Xiaohui},
	month = jul,
	year = {2023},
	note = {ISSN: 1945-788X},
	pages = {174--179},
}

@article{seo_plain_2022,
	title = {Plain {Template} {Insertion}: {Korean}-{Prompt}-{Based} {Engineering} for {Few}-{Shot} {Learners}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3213027},
	abstract = {Prompt-based learning is a method used for language models to interpret natural language by remembering the prior knowledge acquired and the training objective. Recent prompt-based few-shot learners have achieved superior performance by alleviating the catastrophic forgetting that occurs in pretrained language models. Few-shot learning contributes towards solving the data scarcity problem, an enormous challenge in AI systems and a significant consideration in natural language processing research. In spite of the significance of few-shot learning, research on Korean language-based few-shot learning is insufficient, and whether the prompt-based approach is appropriate for the Korean language has not been thoroughly verified. As a step toward realizing a Korean-prompt-based few-shot learner, we attempt to apply prompt engineering to the Korean language understanding benchmark dataset and introduce plain template insertion to overcome data scarcity in a more practical few-shot setting. The contributions of this study are as follows: (1) presumably, this is the first study to apply prompt-based few-shot learning to Korean benchmark datasets. With 32 few-shot settings, it improves performance by +14.88, +29.04, and +1.81 in the natural language inference, semantic textual similarity, and topic classification tasks. (2) We present prompt engineering, which merely inserts a plain template and increases data efficiency without training example selection, augmentation, reformulation, and retrieval. (3) Our approach is robust to the Korean prompt’s contextual information and sentence structure and is applicable to both hard- and soft-prompt.},
	journal = {IEEE Access},
	author = {Seo, Jaehyung and Moon, Hyeonseok and Lee, Chanhee and Eo, Sugyeong and Park, Chanjun and Kim, Jihoon and Chun, Changwoo and Lim, Heuiseok},
	year = {2022},
	pages = {107587--107597},
}

@inproceedings{papa_use_2023,
	title = {On the use of {Stable} {Diffusion} for creating realistic faces: from generation to detection},
	doi = {10.1109/IWBF57495.2023.10156981},
	abstract = {The mass adoption of diffusion models has shown that artificial intelligence (AI) systems can be used to easily generate realistic images. The spread of these technologies paves the way to previously unimaginable creative uses while also raising the possibility of malicious applications. In this work, we propose a critical analysis of the overall pipeline, i.e., from creating realistic human faces with Stable Diffusion v1.5 [1] to recognizing fake ones. We first propose an analysis of the prompts that allow the generation of extremely realistic faces with a human-in-the-loop approach. Our objective is to identify the text prompts that drive the image generation process to obtain realistic photos that resemble everyday portraits captured with any camera. Next, we study how complex it is to recognize these fake contents for both AI-based models and non-expert humans. We conclude that similar to other deepzfake creation techniques, despite some limitations in generalization across different datasets, it is possible to use AI to recognize these contents more accurately than non-expert humans would.},
	booktitle = {2023 11th {International} {Workshop} on {Biometrics} and {Forensics} ({IWBF})},
	author = {Papa, Lorenzo and Faiella, Lorenzo and Corvitto, Luca and Maiano, Luca and Amerini, Irene},
	month = apr,
	year = {2023},
	pages = {1--6},
}

@inproceedings{du_learning_2022,
	title = {Learning to {Prompt} for {Open}-{Vocabulary} {Object} {Detection} with {Vision}-{Language} {Model}},
	doi = {10.1109/CVPR52688.2022.01369},
	abstract = {Recently, vision-language pre-training shows great potential in open-vocabulary object detection, where detectors trained on base classes are devised for detecting new classes. The class text embedding is firstly generated by feeding prompts to the text encoder of a pre-trained vision-language model. It is then used as the region classifier to supervise the training of a detector. The key element that leads to the success of this model is the proper prompt, which requires careful words tuning and ingenious design. To avoid laborious prompt engineering, there are some prompt representation learning methods being proposed for the image classification task, which however can only be sub-optimal solutions when applied to the detection task. In this paper, we introduce a novel method, detection prompt (DetPro), to learn continuous prompt representations for open-vocabulary object detection based on the pre-trained vision-language model. Different from the previous classification-oriented methods, DetPro has two highlights: 1) a background interpretation scheme to include the proposals in image background into the prompt training; 2) a context grading scheme to separate proposals in image foreground for tailored prompt training. We assemble DetPro with ViLD, a recent state-of-the-art openworld object detector, and conduct experiments on the LVIS as well as transfer learning on the Pascal VOC, COCO, Objects365 datasets. Experimental results show that our DetPro outperforms the baseline ViLD [7] in all settings, e.g., +3.4 APbox and +3.0 APmask improvements on the novel classes of LVIS. Code and models are available at https://github.com/dyabel/detpro.},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Du, Yu and Wei, Fangyun and Zhang, Zihe and Shi, Miaojing and Gao, Yue and Li, Guoqi},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	pages = {14064--14073},
}

@inproceedings{shahid_unsupervised_2023,
	title = {Unsupervised {Dual} {Modality} {Prompt} {Learning} for {Facial} {Expression} {Recognition}},
	doi = {10.1109/ICCCS57501.2023.10151313},
	abstract = {A method of facial expression recognition using a vision language model is proposed. Recently vision-language models for example CLIP (Contrastive Language-Image Pre-training) models developed by OpenAI have achieved exceptional results on a variety of image recognition and retrieval tasks, exhibiting strong zero-shot performance. Transferable representations can be adapted through prompt tuning to a variety of downstream tasks. From the general knowledge stored in a pre-trained model, prompt tuning attempts to extract useful information for downstream tasks. In order to avoid time-consuming prompt engineering, recent works use a small amount of labeled data for adapting vision language models to downstream image recognition problems. However, requiring target datasets to be labeled may restrict their scalability. Moreover, we also note that adapting prompt learning techniques in only one branch of CLIP (vision or language) is suboptimal because it won't allow for the dynamic adjustment of both representation spaces on a downstream task. In this paper, we evaluated the performance of the CLIP model as a zero-shot face recognizer and proposed an Unsupervised Dual Modality Prompt Learning framework for Facial Expression Recognition. Our model tunes the prompts through learning text and visual prompts simultaneously to improve alignment between the linguistic and visual representations when labels are not provided for the target dataset. The experimental results on CK+, JAFFE, RAF-DB, and FER2013 datasets showed that our proposed method performs better compared with CLIP Zero-Shot and other unsupervised prompt-based learning methods for facial expression recognition tasks.},
	booktitle = {2023 8th {International} {Conference} on {Computer} and {Communication} {Systems} ({ICCCS})},
	author = {Shahid, Muhammad},
	month = apr,
	year = {2023},
	pages = {1056--1061},
}

@article{giray_prompt_2023,
	title = {Prompt {Engineering} with {ChatGPT}: {A} {Guide} for {Academic} {Writers}},
	issn = {0090-6964},
	doi = {10.1007/s10439-023-03272-4},
	abstract = {Prompt engineering is a relatively new discipline that refers to the practice of developing and optimizing prompts to effectively utilize large language models, particularly in natural language processing tasks. However, not many writers and researchers are familiar about this discipline. Hence, in this paper, I aim to highlight the significance of prompt engineering for academic writers and researchers, particularly the fledgling, in the rapidly evolving world of artificial intelligence. I also discuss the concepts of prompt engineering, large language models, and the techniques and pitfalls of writing prompts. Here, I contend that by acquiring prompt engineering skills, academic writers can navigate the changing landscape and leverage large language models to enhance their writing process. As artificial intelligence continues to advance and penetrate the arena of academic writing, prompt engineering equips writers and researchers with the essential skills to effectively harness the power of language models. This enables them to confidently explore new opportunities, enhance their writing endeavors, and remain at the forefront of utilizing cutting-edge technologies in their academic pursuits.},
	language = {English},
	journal = {ANNALS OF BIOMEDICAL ENGINEERING},
	author = {Giray, Louie},
	month = jun,
	year = {2023},
	note = {Place: ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
Publisher: SPRINGER
Type: Article; Early Access},
	keywords = {Academic writing, ChatGPT, Large language models, Natural language processing, Prompt engineering, Prompts, artificial intelligence, natural language processing, Large language model, Artificial intelligence, Language model, Prompt, Computational linguistics, Natural language processing systems, Language processing, Natural languages, human, skill, human experiment, Academic writings, Engineering skills, fledgling, letter, writing},
	annote = {Cited by: 3},
}

@article{mesko_prompt_2023,
	title = {Prompt {Engineering} as an {Important} {Emerging} {Skill} for {Medical} {Professionals}: {Tutorial}},
	volume = {25},
	issn = {1438-8871},
	doi = {10.2196/50638},
	abstract = {Prompt engineering is a relatively new field of research that refers to the practice of designing, refining, and implementing prompts or instructions that guide the output of large language models (LLMs) to help in various tasks. With the emergence of LLMs, the most popular one being ChatGPT that has attracted the attention of over a 100 million users in only 2 months, artificial intelligence (AI), especially generative AI, has become accessible for the masses. This is an unprecedented paradigm shift not only because of the use of AI becoming more widespread but also due to the possible implications of LLMs in health care. As more patients and medical professionals use AI-based tools, LLMs being the most popular representatives of that group, it seems inevitable to address the challenge to improve this skill. This paper summarizes the current state of research about prompt engineering and, at the same time, aims at providing practical recommendations for the wide range of health care professionals to improve their interactions with LLMs.},
	language = {English},
	journal = {JOURNAL OF MEDICAL INTERNET RESEARCH},
	author = {Mesko, Bertalan},
	month = oct,
	year = {2023},
	note = {Place: 130 QUEENS QUAY East, Unit 1100, TORONTO, ON M5A 0P6, CANADA
Publisher: JMIR PUBLICATIONS, INC
Type: Article},
	keywords = {ChatGPT, AI, AI tool, artificial intelligence, chatbot, chatbots, conversational agent, conversational agents, decision-making, digital health, engineering, future, GPT-4, healthcare professional, language model, large language models, LLM, LLMs, natural language processing, NLP, prompt, prompt engineering, prompts, technology, Language, adult, article, Artificial Intelligence, decision making, Engineering, health care personnel, Health Personnel, human, Humans, language, skill},
	annote = {Cited by: 1; All Open Access, Gold Open Access},
}

@article{zhang_semantic_2024,
	title = {Semantic understanding and prompt engineering for large-scale traffic data imputation},
	volume = {102},
	issn = {1566-2535},
	doi = {10.1016/j.inffus.2023.102038},
	abstract = {Intelligent Transportation Systems (ITS) face the formidable challenge of large-scale missing data, particularly in the imputation of traffic data. Existing studies have mainly relied on modeling network-level spatiotemporal correlations to address this issue. However, these methods often overlook the rich semantic information (e.g., road infrastructure, sensor location, etc.) inherent in road networks when capturing network-wide spatiotemporal correlations. We address this limitation by presenting the Graph Transformer-based Traffic Data Imputation (GT-TDI) model, which imputes missing values in extensive traffic data by leveraging spatiotemporal semantic understanding of road networks. The proposed model leverages semantic descriptions that capture the spatial and temporal dynamics of traffic across road networks, enhancing its capacity to infer comprehensive spatiotemporal relationships. Moreover, to augment the model's capabilities, we employ a Large Language Model (LLM) and prompt engineering to enable natural and intuitive interactions with the traffic data imputation system, allowing users to query and request in plain language, without requiring expert knowledge or complex mathematical models. The proposed model, GT-TDI, utilizes Graph Neural Networks (GNN) and Transformer architectures to perform large-scale traffic data imputation using deficient observations, sensor social connectivity, and semantic descriptions as inputs. We evaluate the GT-TDI model on the PeMS freeway dataset and benchmark it against cutting-edge models. The experimental evidence demonstrates that GT-TDI surpasses the cutting-edge approaches in scenarios with intricate patterns and varying rates of missing data.},
	language = {English},
	journal = {INFORMATION FUSION},
	author = {Zhang, Kunpeng and Zhou, Feng and Wu, Lan and Xie, Na and He, Zhengbing},
	month = feb,
	year = {2024},
	note = {Place: RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
Publisher: ELSEVIER
Type: Article},
	keywords = {Prompt engineering, Graph neural network, Large language model, Semantic understanding, Traffic data imputation, Transformer, Language model, Computational linguistics, Semantics, Large-scales, Cutting tools, Data imputation, Graph neural networks, Intelligent systems, Intelligent vehicle highway systems, Motor transportation, Roads and streets, Search engines, Semantic Web, Semantics understanding, Traffic data},
	annote = {Cited by: 0},
}

@article{lo_clear_2023,
	title = {The {CLEAR} path: {A} framework for enhancing information literacy through prompt engineering},
	volume = {49},
	issn = {0099-1333},
	doi = {10.1016/j.acalib.2023.102720},
	abstract = {This article introduces the CLEAR Framework for Prompt Engineering, designed to optimize interactions with AI language models like ChatGPT. The framework encompasses five core principles-Concise, Logical, Explicit, Adaptive, and Reflective-that facilitate more effective AI-generated content evaluation and creation. Additionally, the article discusses technical aspects of prompts, such as tokens, temperature, and top-p settings. By integrating the CLEAR Framework into infor-mation literacy instruction, academic librarians can empower students with critical thinking skills for the ChatGPT era and adapt to the rapidly evolving AI landscape in higher education.},
	language = {English},
	number = {4},
	journal = {JOURNAL OF ACADEMIC LIBRARIANSHIP},
	author = {Lo, Leo S.},
	month = jul,
	year = {2023},
	note = {Place: STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA
Publisher: ELSEVIER SCIENCE INC
Type: Article},
	annote = {Cited by: 8},
}

@article{kleinig_how_2023,
	title = {How to use large language models in ophthalmology: from prompt engineering to protecting confidentiality},
	issn = {0950-222X},
	doi = {10.1038/s41433-023-02772-w},
	language = {English},
	journal = {EYE},
	author = {Kleinig, Oliver and Gao, Christina and Kovoor, Joshua G. and Gupta, Aashray K. and Bacchi, Stephen and Chan, Weng Onn},
	month = oct,
	year = {2023},
	note = {Place: CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
Publisher: SPRINGERNATURE
Type: Editorial Material; Early Access},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@article{yong_prompt_2023,
	title = {Prompt engineering for zero-shot and few-shot defect detection and classification using a visual-language pretrained model},
	volume = {38},
	issn = {1093-9687},
	doi = {10.1111/mice.12954},
	abstract = {Zero-shot learning, applied with vision-language pretrained (VLP) models, is expected to be an alternative to existing deep learning models for defect detection, under insufficient dataset. However, VLP models, including contrastive language-image pretraining (CLIP), showed fluctuated performance on prompts (inputs), resulting in research on prompt engineering-optimization of prompts for improving performance. Therefore, this study aims to identify the features of a prompt that can yield the best performance in classifying and detecting building defects using the zero-shot and few-shot capabilities of CLIP. The results reveal the following: (1) domain-specific definitions are better than general definitions and images; (2) a complete sentence is better than a set of core terms; and (3) multimodal information is better than single-modal information. The resulting detection performance using the proposed prompting method outperformed that of existing supervised models.},
	language = {English},
	number = {11},
	journal = {COMPUTER-AIDED CIVIL AND INFRASTRUCTURE ENGINEERING},
	author = {Yong, Gunwoo and Jeon, Kahyun and Gil, Daeyoung and Lee, Ghang},
	month = jul,
	year = {2023},
	note = {Place: 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
Publisher: WILEY
Type: Article},
	keywords = {engineering, Deep learning, Building defects, classification, Defect classification, Defect detection, Defects, detection method, Domain specific, Engineering optimization, Image enhancement, Improving performance, learning, Learning models, Learning systems, Multi-modal information, numerical model, optimization, Performance, performance assessment, Pre-training, Visual languages, Zero-shot learning},
	pages = {1536--1554},
	annote = {Cited by: 2},
}

@article{shuai_rationale-augmented_2023,
	title = {A rationale-augmented {NLP} framework to identify unilateral contractual change risk for construction projects},
	volume = {149},
	issn = {0166-3615},
	url = {https://www.sciencedirect.com/science/article/pii/S0166361523000908},
	doi = {https://doi.org/10.1016/j.compind.2023.103940},
	abstract = {Construction projects often see owners who exert their dominance by modifying the standard contracts without contractors’ prior consent. This can lead to undesirable outcomes for contractors, thus referred to as unilateral contractual change risk (UCCR) in this study. As such, identifying UCCR proactively becomes essential for contractors, particularly in engineering-procurement-construction (EPC) projects where the claim scope is limited. Although natural language processing (NLP) has shown promise in the identification of UCCR, it faces difficulties due to the high cognitive demands required. To fill the gap, we propose a rational-augmented NLP framework that emulates human reasoning to identify UCCR in EPC contracts in an explainable and effective manner. The framework (1) leverages NLP techniques to disassemble contract text into features that draw the attention of human readers for obligation-related comprehension, and (2) generates a coherent sequence of intermediate reasoning steps using a customized Microsoft Excel and-in interface to identify predetermined categories of UCCR. Our framework, which achieves an outstanding F1 score of 0.87, is trained on a widely used standard form of EPC contract. It also presents a user-friendly interface for contractors to discern any intentional or malicious acts committed by the owner during the contract stage. Furthermore, our methodology can be adapted to enhance risk management in other sectors.},
	journal = {Computers in Industry},
	author = {Shuai, Bing},
	year = {2023},
	keywords = {BERT, Contract management, FIDIC, Named entity recognition (NER), Natural language processing (NLP), Risk identification},
	pages = {103940},
}

@article{zhao_chatagri_2023,
	title = {{ChatAgri}: {Exploring} potentials of {ChatGPT} on cross-linguistic agricultural text classification},
	volume = {557},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231223008317},
	doi = {https://doi.org/10.1016/j.neucom.2023.126708},
	abstract = {In the era of sustainable smart agriculture, a vast amount of agricultural news text is posted online, accumulating significant agricultural knowledge. To efficiently access this knowledge, effective text classification techniques are urgently needed. Deep learning approaches, such as fine-tuning strategies on pre-trained language models (PLMs), have shown remarkable performance gains. Nonetheless, these methods face several complex challenges, including limited agricultural training data, poor domain transferability (especially across languages), and complex and expensive deployment of large models. Inspired by the success of recent ChatGPT models (e.g., GPT-3.5, GPT-4), this work explores the potential of applying ChatGPT in the field of agricultural informatization. Various crucial factors, such as prompt construction, answer parsing, and different ChatGPT variants, are thoroughly investigated to maximize its capabilities. A preliminary comparative study is conducted, comparing ChatGPT with PLMs-based fine-tuning methods and PLMs-based prompt-tuning methods. Empirical results demonstrate that ChatGPT effectively addresses the mentioned research challenges and bottlenecks, making it an ideal solution for agricultural text classification. Moreover, ChatGPT achieves comparable performance to existing PLM-based fine-tuning methods, even without fine-tuning on agricultural data samples. We hope this preliminary study could inspire the emergence of a general-purpose AI paradigm for agricultural text processing.},
	journal = {Neurocomputing},
	author = {Zhao, Biao and Jin, Weiqiang and Ser, Javier Del and Yang, Guang},
	year = {2023},
	keywords = {ChatGPT, GPT-4, Agricultural text classification, Generative Pre-trained Transformer (GPT), Very large pre-trained language model},
	pages = {126708},
}

@article{sohail_decoding_2023,
	title = {Decoding {ChatGPT}: {A} taxonomy of existing research, current challenges, and possible future directions},
	volume = {35},
	issn = {1319-1578},
	url = {https://www.sciencedirect.com/science/article/pii/S131915782300229X},
	doi = {https://doi.org/10.1016/j.jksuci.2023.101675},
	abstract = {Chat Generative Pre-trained Transformer (ChatGPT) has gained significant interest and attention since its launch in November 2022. It has shown impressive performance in various domains, including passing exams and creative writing. However, challenges and concerns related to biases and trust persist. In this work, we present a comprehensive review of over 100 Scopus-indexed publications on ChatGPT, aiming to provide a taxonomy of ChatGPT research and explore its applications. We critically analyze the existing literature, identifying common approaches employed in the studies. Additionally, we investigate diverse application areas where ChatGPT has found utility, such as healthcare, marketing and financial services, software engineering, academic and scientific writing, research and education, environmental science, and natural language processing. Through examining these applications, we gain valuable insights into the potential of ChatGPT in addressing real-world challenges. We also discuss crucial issues related to ChatGPT, including biases and trustworthiness, emphasizing the need for further research and development in these areas. Furthermore, we identify potential future directions for ChatGPT research, proposing solutions to current challenges and speculating on expected advancements. By fully leveraging the capabilities of ChatGPT, we can unlock its potential across various domains, leading to advancements in conversational AI and transformative impacts in society.},
	number = {8},
	journal = {Journal of King Saud University - Computer and Information Sciences},
	author = {Sohail, Shahab Saquib and Farhat, Faiza and Himeur, Yassine and Nadeem, Mohammad and Madsen, Dag Øivind and Singh, Yashbir and Atalla, Shadi and Mansoor, Wathiq},
	year = {2023},
	keywords = {ChatGPT, Generative Pre-trained Transformer (GPT), AI Generated Content (AIGC), Large language models (LLMs), Systematic review, Trustworthy AI},
	pages = {101675},
}

@article{zhu_prompt_2023,
	title = {A prompt model with combined semantic refinement for aspect sentiment analysis},
	volume = {60},
	issn = {0306-4573},
	url = {https://www.sciencedirect.com/science/article/pii/S0306457323001991},
	doi = {https://doi.org/10.1016/j.ipm.2023.103462},
	abstract = {Recently, pre-trained language models (PLMs), especially pre-trained bidirectional encoder representations from transformers (BERT), have improved the performance of aspect-based sentiment analysis (ABSA) tasks to some extent. However, due to the imbalance of training data in different polarities, the following shortcomings remain in PLM-based ABSA methods: (1) for small corpus scenarios with polarized emotions, an unbalanced performance problem exists; and (2) for delicate and obscure scenes dominated by neutral emotions, PLM-based performance gains are limited. To address these shortcomings, we use BERT as an instance of PLMs to propose a general-purpose prompt model with combined semantic refinement for ABSA. First, we utilize a BERT without fine-tuning to automatically induce prompts for various ABSA datasets to enhance the adaptability of the model to different application scenarios. We then leverage multi-prompt learning to propose a data augmentation method to address the imbalance of training data in different polarities. Moreover, to further deepen the model's understanding and analysis of reviews with prompts, we also propose an improved BERT semantic refinement method that combines global semantic refinement and local semantic extraction. Experiments on five public datasets show that compared with existing methods, our macro-average F1 improvement is over 10\% on polarized small datasets and over 7\% on an emotionally delicate and obscure dataset.},
	number = {5},
	journal = {Information Processing \& Management},
	author = {Zhu, Xinhua and Kuang, Zhongjie and Zhang, Lanfang},
	year = {2023},
	keywords = {BERT, Aspect-based sentiment analysis, Data augmentation, Pre-trained language models, Prompt learning, Semantic refinement},
	pages = {103462},
}

@article{zhang_challenges_2024,
	title = {On the challenges and perspectives of foundation models for medical image analysis},
	volume = {91},
	issn = {1361-8415},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841523002566},
	doi = {https://doi.org/10.1016/j.media.2023.102996},
	abstract = {This article discusses the opportunities, applications and future directions of large-scale pretrained models, i.e., foundation models, which promise to significantly improve the analysis of medical images. Medical foundation models have immense potential in solving a wide range of downstream tasks, as they can help to accelerate the development of accurate and robust models, reduce the dependence on large amounts of labeled data, preserve the privacy and confidentiality of patient data. Specifically, we illustrate the “spectrum” of medical foundation models, ranging from general imaging models, modality-specific models, to organ/task-specific models, and highlight their challenges, opportunities and applications. We also discuss how foundation models can be leveraged in downstream medical tasks to enhance the accuracy and efficiency of medical image analysis, leading to more precise diagnosis and treatment decisions.},
	journal = {Medical Image Analysis},
	author = {Zhang, Shaoting and Metaxas, Dimitris},
	year = {2024},
	keywords = {Foundation models},
	pages = {102996},
}

@article{wang_msprompt_2023,
	title = {{MsPrompt}: {Multi}-step prompt learning for debiasing few-shot event detection},
	volume = {60},
	issn = {0306-4573},
	url = {https://www.sciencedirect.com/science/article/pii/S0306457323002467},
	doi = {https://doi.org/10.1016/j.ipm.2023.103509},
	abstract = {Event detection (ED) is aimed to identify the key trigger words in unstructured text and predict the event types accordingly. Traditional ED models are too data-hungry to accommodate real applications with scarce labeled data. Besides, typical ED models are facing the context-bypassing and disabled generalization issues caused by the trigger bias stemming from ED datasets. Therefore, we focus on the true few-shot paradigm targeting to construct a novel training set that accommodates the low-resource scenarios. In particular, we propose a multi-step prompt learning model (MsPrompt) for debiasing few-shot event detection (FSED), consisting of the following two components: a multi-step prompt module equipped with a knowledge-enhanced ontology to leverage the event semantics and latent prior knowledge in the pretrained language models (PLMs) sufficiently for tackling the context-bypassing problem, and a prototypical network module compensating for the weakness of classifying events with sparse data and boost the generalization performance. Experiments on two public datasets ACE-2005 and FewEvent show that MsPrompt can outperform the state-of-the-art models, especially in the strict low-resource scenarios reporting 11.43\% improvement in terms of weighted F1-score against the best baseline and achieving an outstanding debiasing performance.},
	number = {6},
	journal = {Information Processing \& Management},
	author = {Wang, Siyuan and Zheng, Jianming and Cai, Fei and Song, Chengyu and Luo, Xueshan},
	year = {2023},
	keywords = {Prompt learning, Debiasing, Few-shot event detection},
	pages = {103509},
}

@article{yang_impact_2023,
	title = {The impact of {ChatGPT} and {LLMs} on medical imaging stakeholders: {Perspectives} and use cases},
	volume = {1},
	issn = {2950-1628},
	url = {https://www.sciencedirect.com/science/article/pii/S2950162823000073},
	doi = {https://doi.org/10.1016/j.metrad.2023.100007},
	abstract = {This study investigates the transformative potential of Large Language Models (LLMs), such as OpenAI ChatGPT, in medical imaging. With the aid of public data, these models, which possess remarkable language understanding and generation capabilities, are augmenting the interpretive skills of radiologists, enhancing patient-physician communication, and streamlining clinical workflows. The paper introduces an analytic framework for presenting the complex interactions between LLMs and the broader ecosystem of medical imaging stakeholders, including businesses, insurance entities, governments, research institutions, and hospitals (nicknamed BIGR-H). Through detailed analyses, illustrative use cases, and discussions on the broader implications and future directions, this perspective seeks to raise discussion in strategic planning and decision-making in the era of AI-enabled healthcare.},
	number = {1},
	journal = {Meta-Radiology},
	author = {Yang, Jiancheng and Li, Hongwei Bran and Wei, Donglai},
	year = {2023},
	keywords = {ChatGPT, LLM, Foundation models, Medical imaging},
	pages = {100007},
}

@article{shi_robust_2024,
	title = {Robust scientific text classification using prompt tuning based on data augmentation with {L2} regularization},
	volume = {61},
	issn = {0306-4573},
	url = {https://www.sciencedirect.com/science/article/pii/S0306457323002686},
	doi = {https://doi.org/10.1016/j.ipm.2023.103531},
	abstract = {Recently, the prompt tuning technique, which incorporates prompts into the input of the pre-training language model (like BERT, GPT), has shown promise in improving the performance of language models when facing limited annotated data. However, the equivalence of template semantics in learning is not related to the effect of prompts and the prompt tuning often exhibits unstable performance, which is more severe in the domain of the scientific domain. To address this challenge, we propose to enhance prompt tuning using data augmentation with L2 regularization. Namely, pairing-wise training for the pair of the original and transformed data is performed. Our experiments on two scientific text datasets (ACL-ARC and SciCite) demonstrate that our proposed method significantly improves both accuracy and robustness. By using 1000 samples out of 1688 in the ACL-ARC training set, our method achieved an F1 score 3.33\% higher than the same model trained on all 1688-sample data. In the SciCite dataset, our method surpassed the same model with labeled data reduced by over 93\%. Our method is also proved to have high robustness, reaching F1 scores from 1\% to 8\% higher than those models without our method after the Probability Weighted Word Saliency attack.},
	number = {1},
	journal = {Information Processing \& Management},
	author = {Shi, Shijun and Hu, Kai and Xie, Jie and Guo, Ya and Wu, Huayi},
	year = {2024},
	keywords = {Data augmentation, L2 regularization, Pairwise training, Pre-training model, Prompt tuning, Scientific text classification},
	pages = {103531},
}

@article{scanlon_chatgpt_2023,
	title = {{ChatGPT} for digital forensic investigation: {The} good, the bad, and the unknown},
	volume = {46},
	issn = {2666-2817},
	url = {https://www.sciencedirect.com/science/article/pii/S266628172300121X},
	doi = {https://doi.org/10.1016/j.fsidi.2023.301609},
	abstract = {The disruptive application of ChatGPT (GPT-3.5, GPT-4) to a variety of domains has become a topic of much discussion in the scientific community and society at large. Large Language Models (LLMs), e.g., BERT, Bard, Generative Pre-trained Transformers (GPTs), LLaMA, etc., have the ability to take instructions, or prompts, from users and generate answers and solutions based on very large volumes of text-based training data. This paper assesses the impact and potential impact of ChatGPT on the field of digital forensics, specifically looking at its latest pre-trained LLM, GPT-4. A series of experiments are conducted to assess its capability across several digital forensic use cases including artefact understanding, evidence searching, code generation, anomaly detection, incident response, and education. Across these topics, its strengths and risks are outlined and a number of general conclusions are drawn. Overall this paper concludes that while there are some potential low-risk applications of ChatGPT within digital forensics, many are either unsuitable at present, since the evidence would need to be uploaded to the service, or they require sufficient knowledge of the topic being asked of the tool to identify incorrect assumptions, inaccuracies, and mistakes. However, to an appropriately knowledgeable user, it could act as a useful supporting tool in some circumstances.},
	journal = {Forensic Science International: Digital Investigation},
	author = {Scanlon, Mark and Breitinger, Frank and Hargreaves, Christopher and Hilgert, Jan-Niclas and Sheppard, John},
	year = {2023},
	keywords = {ChatGPT, Artificial intelligence, Digital forensics, Generative pre-trained transformers (GPT), Large language models (LLM)},
	pages = {301609},
}

@article{wang_trustworthy_2022,
	title = {Trustworthy assertion classification through prompting},
	volume = {132},
	issn = {1532-0464},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046422001538},
	doi = {https://doi.org/10.1016/j.jbi.2022.104139},
	abstract = {Accurate identification of the presence, absence or possibility of relevant entities in clinical notes is important for healthcare professionals to quickly understand crucial clinical information. This introduces the task of assertion classification - to correctly identify the assertion status of an entity in the unstructured clinical notes. Recent rule-based and machine-learning approaches suffer from labor-intensive pattern engineering and severe class bias toward majority classes. To solve this problem, in this study, we propose a prompt-based learning approach, which treats the assertion classification task as a masked language auto-completion problem. We evaluated the model on six datasets. Our prompt-based method achieved a micro-averaged F-1 of 0.954 on the i2b2 2010 assertion dataset, with ∼1.8\% improvements over previous works. In particular, our model showed excellence in detecting classes with few instances (few-shot). Evaluations on five external datasets showcase the outstanding generalizability of the prompt-based method to unseen data. To examine the rationality of our model, we further introduced two rationale faithfulness metrics: comprehensiveness and sufficiency. The results reveal that compared to the “pre-train, fine-tune” procedure, our prompt-based model has a stronger capability of identifying the comprehensive (∼63.93\%) and sufficient (∼11.75\%) linguistic features from free text. We further evaluated the model-agnostic explanations using LIME. The results imply a better rationale agreement between our model and human beings (∼71.93\% in average F-1), which demonstrates the superior trustworthiness of our model.},
	journal = {Journal of Biomedical Informatics},
	author = {Wang, Song and Tang, Liyan and Majety, Akash and Rousseau, Justin F. and Shih, George and Ding, Ying and Peng, Yifan},
	year = {2022},
	keywords = {NLP, Concept assertion, Deep learning, Prompt-based learning},
	pages = {104139},
}

@article{han_f-scp_2024,
	title = {F-{SCP}: {An} automatic prompt generation method for specific classes based on visual language pre-training models},
	volume = {147},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320323007938},
	doi = {https://doi.org/10.1016/j.patcog.2023.110096},
	abstract = {The zero-shot classification performance of large-scale vision-language pre-training models (e.g., CLIP, BLIP and ALIGN) can be enhanced by incorporating a prompt (e.g., “a photo of a [CLASS]”) before the class words. Modifying the prompt slightly can have significant effect on the classification outcomes of these models. Thus, it is crucial to include an appropriate prompt tailored to the classes. However, manual prompt design is labor-intensive and necessitates domain-specific expertise. The CoOp (Context Optimization) converts hand-crafted prompt templates into learnable word vectors to automatically generate prompts, resulting in substantial improvements for CLIP. However, CoOp exhibited significant variation in classification performance across different classes. Although CoOp-CSC (Class-Specific Context) has a separate prompt for each class, only shows some advantages on fine-grained datasets. In this paper, we propose a novel automatic prompt generation method called F-SCP (Filter-based Specific Class Prompt), which distinguishes itself from the CoOp-UC (Unified Context) model and the CoOp-CSC model. Our approach focuses on prompt generation for low-accuracy classes and similar classes. We add the Filter and SCP modules to the prompt generation architecture. The Filter module selects the poorly classified classes, and then reproduce the prompts through the SCP (Specific Class Prompt) module to replace the prompts of specific classes. Experimental results on six multi-domain datasets shows the superiority of our approach over the state-of-the-art methods. Particularly, the improvement in accuracy for the specific classes mentioned above is significant. For instance, compared with CoOp-UC on the OxfordPets dataset, the low-accuracy classes, such as, Class21 and Class26, are improved by 18\% and 12\%, respectively.},
	journal = {Pattern Recognition},
	author = {Han, Baihong and Jiang, Xiaoyan and Fang, Zhijun and Fujita, Hamido and Gao, Yongbin},
	year = {2024},
	keywords = {Prompt tuning, Large-scale pre-training model, Multi-modal, Vision language model},
	pages = {110096},
}

@article{wang_performance_2023,
	title = {Performance and exploration of {ChatGPT} in medical examination, records and education in {Chinese}: {Pave} the way for medical {AI}},
	volume = {177},
	issn = {1386-5056},
	url = {https://www.sciencedirect.com/science/article/pii/S1386505623001910},
	doi = {https://doi.org/10.1016/j.ijmedinf.2023.105173},
	abstract = {Background Although chat generative pre-trained transformer (ChatGPT) has made several successful attempts in the medical field, most notably in answering medical questions in English, no studies have evaluated ChatGPT's performance in a Chinese context for a medical task. Objective The aim of this study was to evaluate ChatGPT’s ability to understand medical knowledge in Chinese, as well as its potential to serve as an electronic health infrastructure for medical development, by evaluating its performance in medical examinations, records, and education. Method The Chinese (CNMLE) and English (ENMLE) datasets of the China National Medical Licensing Examination and the Chinese dataset (NEEPM) of the China National Entrance Examination for Postgraduate Clinical Medicine Comprehensive Ability were used to evaluate the performance of ChatGPT (GPT-3.5 and GPT-4). We assessed answer accuracy, verbal fluency, and the classification of incorrect responses owing to hallucinations on multiple occasions. In addition, we tested ChatGPT's performance on discharge summaries and group learning in a Chinese context on a small scale. Results The accuracy of GPT-3.5 in CNMLE, ENMLE, and NEEPM was 56\% (56/100), 76\% (76/100), and 62\% (62/100), respectively, compared to that of GPT-4, which was of 84\% (84/100), 86\% (86/100), and 82\% (82/100). The verbal fluency of all the ChatGPT responses exceeded 95\%. Among the GPT-3.5 incorrect responses, the proportions of open-domain hallucinations were 66 \% (29/44), 54 \% (14/24), and 63 \% (24/38), whereas close-domain hallucinations accounted for 34 \% (15/44), 46 \% (14/24), and 37 \% (14/38), respectively. By contrast, GPT-4 open-domain hallucinations accounted for 56\% (9/16), 43\% (6/14), and 83\% (15/18), while close-domain hallucinations accounted for 44\% (7/16), 57\% (8/14), and 17\% (3/18), respectively. In the discharge summary, ChatGPT demonstrated logical coherence, however GPT-3.5 could not fulfill the quality requirements, while GPT-4 met the qualification of 60\% (6/10). In group learning, the verbal fluency and interaction satisfaction with ChatGPT were 100\% (10/10). Conclusion ChatGPT based on GPT-4 is at par with Chinese medical practitioners who passed the CNMLE and at the standard required for admission to clinical medical graduate programs in China. The GPT-4 shows promising potential for discharge summarization and group learning. Additionally, it shows high verbal fluency, resulting in a positive human–computer interaction experience. GPT-4 significantly improves multiple capabilities and reduces hallucinations compared to the previous GPT-3.5 model, with a particular leap forward in the Chinese comprehension capability of medical tasks. Artificial intelligence (AI) systems face the challenges of hallucinations, legal risks, and ethical issues. However, we discovered ChatGPT's potential to promote medical development as an electronic health infrastructure, paving the way for Medical AI to become necessary.},
	journal = {International Journal of Medical Informatics},
	author = {Wang, Hongyan and Wu, WeiZhen and Dou, Zhi and He, Liangliang and Yang, Liqiang},
	year = {2023},
	pages = {105173},
}

@article{ray_chatgpt_2023,
	title = {{ChatGPT}: {A} comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope},
	volume = {3},
	issn = {2667-3452},
	url = {https://www.sciencedirect.com/science/article/pii/S266734522300024X},
	doi = {https://doi.org/10.1016/j.iotcps.2023.04.003},
	abstract = {In recent years, artificial intelligence (AI) and machine learning have been transforming the landscape of scientific research. Out of which, the chatbot technology has experienced tremendous advancements in recent years, especially with ChatGPT emerging as a notable AI language model. This comprehensive review delves into the background, applications, key challenges, and future directions of ChatGPT. We begin by exploring its origins, development, and underlying technology, before examining its wide-ranging applications across industries such as customer service, healthcare, and education. We also highlight the critical challenges that ChatGPT faces, including ethical concerns, data biases, and safety issues, while discussing potential mitigation strategies. Finally, we envision the future of ChatGPT by exploring areas of further research and development, focusing on its integration with other technologies, improved human-AI interaction, and addressing the digital divide. This review offers valuable insights for researchers, developers, and stakeholders interested in the ever-evolving landscape of AI-driven conversational agents. This study explores the various ways ChatGPT has been revolutionizing scientific research, spanning from data processing and hypothesis generation to collaboration and public outreach. Furthermore, the paper examines the potential challenges and ethical concerns surrounding the use of ChatGPT in research, while highlighting the importance of striking a balance between AI-assisted innovation and human expertise. The paper presents several ethical issues in existing computing domain and how ChatGPT can invoke challenges to such notion. This work also includes some biases and limitations of ChatGPT. It is worth to note that despite of several controversies and ethical concerns, ChatGPT has attracted remarkable attentions from academia, research, and industries in a very short span of time.},
	journal = {Internet of Things and Cyber-Physical Systems},
	author = {Ray, Partha Pratim},
	year = {2023},
	keywords = {ChatGPT, Natural language processing, Context understanding, Conversational AI, Generative AI, GPT-3.5, Language model},
	pages = {121--154},
}

@article{shi_leveraging_2023,
	title = {Leveraging {GPT}-4 for food effect summarization to enhance product-specific guidance development via iterative prompting},
	volume = {148},
	issn = {1532-0464},
	url = {https://www.sciencedirect.com/science/article/pii/S153204642300254X},
	doi = {https://doi.org/10.1016/j.jbi.2023.104533},
	abstract = {Food effect summarization from New Drug Application (NDA) is an essential component of product-specific guidance (PSG) development and assessment, which provides the basis of recommendations for fasting and fed bioequivalence studies to guide the pharmaceutical industry for developing generic drug products. However, manual summarization of food effect from extensive drug application review documents is time-consuming. Therefore, there is a need to develop automated methods to generate food effect summary. Recent advances in natural language processing (NLP), particularly large language models (LLMs) such as ChatGPT and GPT-4, have demonstrated great potential in improving the effectiveness of automated text summarization, but its ability with regard to the accuracy in summarizing food effect for PSG assessment remains unclear. In this study, we introduce a simple yet effective approach,iterative prompting, which allows one to interact with ChatGPT or GPT-4 more effectively and efficiently through multi-turn interaction. Specifically, we propose a three-turn iterative prompting approach to food effect summarization in which the keyword-focused and length-controlled prompts are respectively provided in consecutive turns to refine the quality of the generated summary. We conduct a series of extensive evaluations, ranging from automated metrics to FDA professionals and even evaluation by GPT-4, on 100 NDA review documents selected over the past five years. We observe that the summary quality is progressively improved throughout the iterative prompting process. Moreover, we find that GPT-4 performs better than ChatGPT, as evaluated by FDA professionals (43\% vs. 12\%) and GPT-4 (64\% vs. 35\%). Importantly, all the FDA professionals unanimously rated that 85\% of the summaries generated by GPT-4 are factually consistent with the golden reference summary, a finding further supported by GPT-4 rating of 72\% consistency. Taken together, these results strongly suggest a great potential for GPT-4 to draft food effect summaries that could be reviewed by FDA professionals, thereby improving the efficiency of the PSG assessment cycle and promoting generic drug product development.},
	journal = {Journal of Biomedical Informatics},
	author = {Shi, Yiwen and Ren, Ping and Wang, Jing and Han, Biao and ValizadehAslani, Taha and Agbavor, Felix and Zhang, Yi and Hu, Meng and Zhao, Liang and Liang, Hualou},
	year = {2023},
	keywords = {GPT-4, Drug labeling, Large Language Models, Prompt Engineering, Text Summarization},
	pages = {104533},
}

@article{gill_transformative_2024,
	title = {Transformative effects of {ChatGPT} on modern education: {Emerging} {Era} of {AI} {Chatbots}},
	volume = {4},
	issn = {2667-3452},
	url = {https://www.sciencedirect.com/science/article/pii/S2667345223000354},
	doi = {https://doi.org/10.1016/j.iotcps.2023.06.002},
	abstract = {ChatGPT, an AI-based chatbot, offers coherent and useful replies based on analysis of large volumes of data. In this article, leading academics, scientists, distinguish researchers and engineers discuss the transformative effects of ChatGPT on modern education. This research discusses ChatGPT capabilities and its use in the education sector, identifies potential concerns and challenges. Our preliminary evaluation shows that ChatGPT perform differently in different subject areas including finance, coding, maths, and general public queries. While ChatGPT has the ability to help educators by creating instructional content, offering suggestions and acting as an online educator to learners by answering questions, transforming education through smartphones and IoT gadgets, and promoting group work, there are clear drawbacks in its use, such as the possibility of producing inaccurate or false data and circumventing duplicate content (plagiarism) detectors where originality is essential. The often reported “hallucinations” within GenerativeAI in general, and also relevant for ChatGPT, can render its use of limited benefit where accuracy is essential. What ChatGPT lacks is a stochastic measure to help provide sincere and sensitive communication with its users. Academic regulations and evaluation practices used in educational institutions need to be updated, should ChatGPT be used as a tool in education. To address the transformative effects of ChatGPT on the learning environment, educating teachers and students alike about its capabilities and limitations will be crucial.},
	journal = {Internet of Things and Cyber-Physical Systems},
	author = {Gill, Sukhpal Singh and Xu, Minxian and Patros, Panos and Wu, Huaming and Kaur, Rupinder and Kaur, Kamalpreet and Fuller, Stephanie and Singh, Manmeet and Arora, Priyansh and Parlikad, Ajith Kumar and Stankovski, Vlado and Abraham, Ajith and Ghosh, Soumya K. and Lutfiyya, Hanan and Kanhere, Salil S. and Bahsoon, Rami and Rana, Omer and Dustdar, Schahram and Sakellariou, Rizos and Uhlig, Steve and Buyya, Rajkumar},
	year = {2024},
	keywords = {ChatGPT, Artificial intelligence, Academics, Chatbot, Education, IoT, Machine learning},
	pages = {19--23},
}

@article{peng_region-adaptive_2023,
	title = {Region-adaptive and context-complementary cross modulation for {RGB}-{T} semantic segmentation},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320323007896},
	doi = {https://doi.org/10.1016/j.patcog.2023.110092},
	abstract = {RGB-Thermal (RGB-T) semantic segmentation is an emerging task aiming to improve the robustness of segmentation methods under extreme imaging conditions with the aid of thermal infrared modality. Foreground-background distinguishment and complementary information mining are two key challenges of this task. Recent methods use naive channel attention and cross-attention to tackle these challenges, but they still struggle with a sub-optimal solution where salient foreground features and noisy background ones might be equally modulated without distinction. The quadratic computational overhead of cross-attention also blocks its application on high-resolution features. Moreover, lacking complementary information mining in the encoding phase hinders the comprehensive scene encoding as well. To alleviate these limitations, we propose a cross modulation process with two collaborative components. The first Region-Adaptive Channel Modulation (RACM) module conducts channel attention at a fine-grained region level where foreground and background regions can be modulated differently in each channel. The second Context-Complementary Spatial Modulation (CCSM) module mines and transfers complementary information between the two modalities early in the encoding phase. Experiments show that our method achieves state-of-the-art performances on current RGB-T segmentation benchmarks.},
	journal = {Pattern Recognition},
	author = {Peng, Fengguang and Ding, Zihan and Chen, Ziming and Wang, Gang and Hui, Tianrui and Liu, Si and Shi, Hang},
	year = {2023},
	keywords = {Context-Complementary Spatial Modulation, Region-Adaptive Channel Modulation, RGB-Thermal, Semantic segmentation},
	pages = {110092},
}

@article{liu_prompt-enhanced_2023,
	title = {Prompt-enhanced hierarchical transformer elevating cardiopulmonary resuscitation instruction via temporal action segmentation},
	issn = {0010-4825},
	url = {https://www.sciencedirect.com/science/article/pii/S001048252301137X},
	doi = {https://doi.org/10.1016/j.compbiomed.2023.107672},
	abstract = {The vast majority of people who suffer unexpected cardiac arrest are performed cardiopulmonary resuscitation (CPR) by passersby in a desperate attempt to restore life, but endeavors turn out to be fruitless on account of disqualification. Fortunately, many pieces of research manifest that disciplined training will help to elevate the success rate of resuscitation, which constantly desires a seamless combination of novel techniques to yield further advancement. To this end, we collect a specialized CPR video dataset in which trainees make efforts to behave resuscitation on mannequins independently in adherence to approved guidelines, promoting an auxiliary toolbox to assist supervision and rectification of intermediate potential issues via modern deep learning methodologies. Our research empirically views this problem as a temporal action segmentation (TAS) task in computer vision, which aims to segment an untrimmed video at a frame-wise level. Here, we propose a Prompt-enhanced hierarchical Transformer (PhiTrans) that integrates three indispensable modules, including a textual prompt-based Video Features Extractor (VFE), a transformer-based Action Segmentation Executor (ASE), and a regression-based Prediction Refinement Calibrator (PRC). The backbone preferentially derives from applications in three approved public datasets (GTEA, 50Salads, and Breakfast) collected for TAS tasks, which experimentally facilitates the model excavation on the CPR dataset. In general, we probe into a feasible pipeline that elevates the CPR instruction qualification via action segmentation equipped with novel deep learning techniques. Associated experiments on the CPR dataset advocate our resolution with surpassing 91.0\% on Accuracy, Edit score, and F1 score.},
	journal = {Computers in Biology and Medicine},
	author = {Liu, Yang and Zhong, Xiaoyun and Zhai, Shiyao and Du, Zhicheng and Gao, Zhenyuan and Huang, Qiming and Zhang, Can Yang and Jiang, Bin and Pandey, Vijay Kumar and Han, Sanyang and Wang, Runming and Han, Yuxing and Wang, Chuhui and Qin, Peiwu},
	year = {2023},
	keywords = {Transformer, Boundary regression refinement, Instructional cardiopulmonary resuscitation, Prompt, Temporal action segmentation},
	pages = {107672},
}

@article{xu_deep_2023,
	title = {Deep image captioning: {A} review of methods, trends and future challenges},
	volume = {546},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231223004101},
	doi = {https://doi.org/10.1016/j.neucom.2023.126287},
	abstract = {Image captioning, also called report generation in medical field, aims to describe visual content of images in human language, which requires to model semantic relationship between visual and textual elements and generate corresponding descriptions that conform to human language cognition. Image captioning is significant for promoting human–computer interaction in all fields and particularly, for computer-aided diagnosis in medical field. Currently, with the rapid development of deep learning technologies, image caption has attracted increasing attention of many researchers in artificial intelligence-related fields. To this end, this study attempts to provide readers with systematic and comprehensive research about different deep image captioning methods in natural and medical fields. We first introduce workflow of image captioning from perspective of simulating human process of describing images, including seeing, focusing and telling, which is respectively behavioralized into feature representation, visual encoding and language generation. Within it, we present common-used feature representation, visual encoding and language generation models. Then, we review datasets, evaluations and basic losses used in image captioning, and summarize typical caption methods which are generally divided into that with or without using reinforcement learning. Besides, we describe advantages and disadvantages of existing methods, and conclusion and challenges are finally presented.},
	journal = {Neurocomputing},
	author = {Xu, Liming and Tang, Quan and Lv, Jiancheng and Zheng, Bochuan and Zeng, Xianhua and Li, Weisheng},
	year = {2023},
	keywords = {Feature representation, Image caption, Language generation, Reinforcement learning, Visual encoding},
	pages = {126287},
}

@article{levantino_generative_2023,
	title = {Generative and {AI}-powered oracles: “{What} will they say about you?”},
	volume = {51},
	issn = {0267-3649},
	url = {https://www.sciencedirect.com/science/article/pii/S0267364923001085},
	doi = {https://doi.org/10.1016/j.clsr.2023.105898},
	abstract = {In less than one year from its launch, the chatbot ChatGPT has captured widespread public attention, thanks to its ease of use and remarkable performance. However, part of this interest is due to its involvement in some data protection and data security issues. In the context of the ongoing debate surrounding similar technologies, such as generative AI, this contribution will first introduce the "ChatGPT phenomenon" (Sections 1 and 2). Then, it will analyse the various positions taken by some key stakeholders on the issues of above and the regulation of the design and use of such technologies, examining these perspectives through the lenses of “Digital Constitutionalism”. Particularly, this paper will emphasise the role that civil society can play in such dynamics (Section 3). Subsequently, it will further promote an active and forward-looking approach in addressing the looming threats these and other AI-based technologies could pose to fundamental rights and society as a whole (Section 4). As we already approach the next AI era without even noticing, the question worth asking ourselves is: “What will generative and AI-powered oracles reveal about us?” (Section 5).},
	journal = {Computer Law \& Security Review},
	author = {Levantino, Francesco Paolo},
	year = {2023},
	keywords = {ChatGPT, Artificial intelligence, Generative AI, Brain-computer interfaces, Data protection, Fundamental rights, Privacy, Virtual reality},
	pages = {105898},
}

@article{song_taxonprompt_2023,
	title = {{TaxonPrompt}: {Taxonomy}-aware curriculum prompt learning for few-shot event classification},
	volume = {264},
	issn = {0950-7051},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705123000400},
	doi = {https://doi.org/10.1016/j.knosys.2023.110290},
	abstract = {Event classification (EC) aims to assign the event labels to unlabeled sentences and tends to struggle in real-world applications when only a few annotated samples are available. Previous studies have mainly focused on using meta-learning to overcome the low-resource problem where label data from other tasks are still required for model learning and selection. Accordingly, prompt learning-based approaches are proposed to address the low-resource issue. However, such approaches generally ignore task-specific information and adopt demonstration learning for fine-tuning, which fails to leverage the most informative examples for training and hurts performance. Thus, we propose a taxonomy-aware prompt learning framework TaxonPrompt that trains the language model with samples from easy to hard by imitating the human curricula, which effectively alleviates the classification bottleneck caused by insufficient data. We first design an event prompt generation (EPG) for automatically generating task-specific templates using sentences, labels, and trigger words. Then, we propose a Fisher information-based demonstration filtering (FDF) to dynamically select the most informative support examples for each query to train the model. We have conducted extensive experiments on two EC datasets: FewEvent and RAMS. The experimental results demonstrate the superiority of the proposed model over state-of-the-art baselines. In particular, our approach works well in the scenario of an extremely small number of available task resources and therefore constitutes a solution for few-shot event classification.},
	journal = {Knowledge-Based Systems},
	author = {Song, Chengyu and Cai, Fei and Wang, Mengru and Zheng, Jianming and Shao, Taihua},
	year = {2023},
	keywords = {Prompt tuning, Event classification, Few-shot learning, Information extraction, Pre-trained language model, Template generation},
	pages = {110290},
}

@article{dakhel_github_2023,
	title = {{GitHub} {Copilot} {AI} pair programmer: {Asset} or {Liability}?},
	volume = {203},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121223001292},
	doi = {https://doi.org/10.1016/j.jss.2023.111734},
	abstract = {Automatic program synthesis is a long-lasting dream in software engineering. Recently, a promising Deep Learning (DL) based solution, called Copilot, has been proposed by OpenAI and Microsoft as an industrial product. Although some studies evaluate the correctness of Copilot solutions and report its issues, more empirical evaluations are necessary to understand how developers can benefit from it effectively. In this paper, we study the capabilities of Copilot in two different programming tasks: (i) generating (and reproducing) correct and efficient solutions for fundamental algorithmic problems, and (ii) comparing Copilot’s proposed solutions with those of human programmers on a set of programming tasks. For the former, we assess the performance and functionality of Copilot in solving selected fundamental problems in computer science, like sorting and implementing data structures. In the latter, a dataset of programming problems with human-provided solutions is used. The results show that Copilot is capable of providing solutions for almost all fundamental algorithmic problems, however, some solutions are buggy and non-reproducible. Moreover, Copilot has some difficulties in combining multiple methods to generate a solution. Comparing Copilot to humans, our results show that the correct ratio of humans’ solutions is greater than Copilot’s suggestions, while the buggy solutions generated by Copilot require less effort to be repaired. Based on our findings, if Copilot is used by expert developers in software projects, it can become an asset since its suggestions could be comparable to humans’ contributions in terms of quality. However, Copilot can become a liability if it is used by novice developers who may fail to filter its buggy or non-optimal solutions due to a lack of expertise.},
	journal = {Journal of Systems and Software},
	author = {Dakhel, Arghavan Moradi and Majdinasab, Vahid and Nikanjam, Amin and Khomh, Foutse and Desmarais, Michel C. and Jiang, Zhen Ming (Jack)},
	year = {2023},
	keywords = {Language model, Code completion, GitHub copilot, Testing},
	pages = {111734},
}

@article{lu_medkpl_2023,
	title = {{MedKPL}: {A} heterogeneous knowledge enhanced prompt learning framework for transferable diagnosis},
	volume = {143},
	issn = {1532-0464},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046423001387},
	doi = {https://doi.org/10.1016/j.jbi.2023.104417},
	abstract = {Artificial Intelligence (AI) based diagnosis systems have emerged as powerful tools to reform traditional medical care. Each clinician now wants to have his own intelligent diagnostic partner to expand the range of services he can provide. However, the implementation of intelligent decision support systems based on clinical note has been hindered by the lack of extensibility of end-to-end AI diagnosis algorithms. When reading a clinical note, expert clinicians make inferences with relevant medical knowledge, which serve as prompts for making accurate diagnoses. Therefore, external medical knowledge is commonly employed as an augmentation for medical text classification tasks. Existing methods, however, cannot integrate knowledge from various knowledge sources as prompts nor can fully utilize explicit and implicit knowledge. To address these issues, we propose a Medical Knowledge-enhanced Prompt Learning (MedKPL) diagnostic framework for transferable clinical note classification. Firstly, to overcome the heterogeneity of knowledge sources, such as knowledge graphs or medical QA databases, MedKPL uniform the knowledge relevant to the disease into text sequences of fixed format. Then, MedKPL integrates medical knowledge into the prompt designed for context representation. Therefore, MedKPL can integrate knowledge into the models to enhance diagnostic performance and effectively transfer to new diseases by using relevant disease knowledge. The results of our experiments on two medical datasets demonstrate that our method yields superior medical text classification results and performs better in cross-departmental transfer tasks under few-shot or even zero-shot settings. These findings demonstrate that our MedKPL framework has the potential to improve the interpretability and transferability of current diagnostic systems.},
	journal = {Journal of Biomedical Informatics},
	author = {Lu, Yuxing and Liu, Xiaohong and Du, Zongxin and Gao, Yuanxu and Wang, Guangyu},
	year = {2023},
	keywords = {Knowledge Integration, Natural Language Processing, Prompt Learning, Text Classification},
	pages = {104417},
}

@article{zong_solving_2023,
	title = {Solving math word problems concerning systems of equations with {GPT} models},
	volume = {14},
	issn = {2666-8270},
	url = {https://www.sciencedirect.com/science/article/pii/S2666827023000592},
	doi = {https://doi.org/10.1016/j.mlwa.2023.100506},
	abstract = {Researchers have been interested in developing AI tools to help students learn various mathematical subjects. One challenging set of tasks for school students is learning to solve math word problems. We explore how recent advances in natural language processing, specifically the rise of powerful transformer based models, can be applied to help math learners with such problems. Concretely, we evaluate the use of GPT-3, GPT-3.5, and GPT-4, all transformer models with billions of parameters recently released by OpenAI, for three related challenges pertaining to math word problems corresponding to systems of two linear equations. The three challenges are classifying word problems, extracting equations from word problems, and generating word problems. For the first challenge, we define a set of problem classes and find that GPT models generally result in classifying word problems with an overall accuracy around 70\%. There is one class that all models struggle about, namely the “item and property” class, which significantly lowered the value. For the second challenge, our findings align with researchers’ expectation: newer models are better at extracting equations from word problems. The highest accuracy we get from fine-tuning GPT-3 with 1000 examples (78\%) is surpassed by GPT-4 given only 20 examples (79\%). For the third challenge, we again find that GPT-4 outperforms the other two models. It is able to generate problems with accuracy ranging from 76.7\% to 100\%, depending on the problem type.},
	journal = {Machine Learning with Applications},
	author = {Zong, Mingyu and Krishnamachari, Bhaskar},
	year = {2023},
	keywords = {AI, Language model, Math education},
	pages = {100506},
}

@article{pham_combined_2023,
	title = {Combined scaling for zero-shot transfer learning},
	volume = {555},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231223007816},
	doi = {https://doi.org/10.1016/j.neucom.2023.126658},
	abstract = {Recent developments in multimodal training methodologies, including CLIP and ALIGN, obviate the necessity for individual data labeling. These approaches utilize pairs of data and corresponding textual information found online as a form of weak supervision signal. However, models employing this kind of weak supervision are not as competitive as their supervised and semi-supervised counterparts when sufficient labeled data is accessible. This performance gap constrains the applicability of weekly supervised models. In this paper, we narrow the gap by proposing a combined scaling method, named BASIC, that achieves 85.7\% top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from any labeled ImageNet example. This accuracy surpasses best-published similar models, CLIP and ALIGN, by 9.3\%. Our BASIC model also shows significant improvements in robustness benchmarks. For instance, on 5 test sets with natural distribution shifts such as ImageNet-A,R,V2,Sketch and ObjectNet, our model achieves 84.3\% top-1 average accuracy, only a small drop from its original ImageNet accuracy. To achieve these results, we first develop a theoretical framework which shows that larger contrastive batch sizes lead to smaller generalization gaps for image-text models such as CLIP and ALIGN. Based on this theoretical result, we scale up the contrastive learning framework of CLIP and ALIGN in three dimensions (data size, model size, and batch size) by proposing a new method using gradient checkpointing and model parallelism. As a result, our dataset has 6.6B noisy image-text pairs, which is 4x larger than ALIGN, and 16x larger than CLIP. Our largest model has 3B weights, which is 3.75x larger in parameters and 8x larger in FLOPs than ALIGN and CLIP. Finally, our batch size is 65536 which is 2x more than CLIP and 4x more than ALIGN.},
	journal = {Neurocomputing},
	author = {Pham, Hieu and Dai, Zihang and Ghiasi, Golnaz and Kawaguchi, Kenji and Liu, Hanxiao and Yu, Adams Wei and Yu, Jiahui and Chen, Yi-Ting and Luong, Minh-Thang and Wu, Yonghui and Tan, Mingxing and Le, Quoc V.},
	year = {2023},
	keywords = {Deep learning, Computer vision, Deep neural networks, Zero-shot transfer},
	pages = {126658},
}

@article{hernandez-bautista_deep_2023,
	title = {Deep learning of curvature features for shape completion},
	volume = {115},
	issn = {0097-8493},
	url = {https://www.sciencedirect.com/science/article/pii/S009784932300136X},
	doi = {https://doi.org/10.1016/j.cag.2023.07.007},
	abstract = {The paper presents a novel solution to the issue of incomplete regions in 3D meshes obtained through digitization. Traditional methods for estimating the surface of missing geometry and topology often yield unrealistic outcomes for intricate surfaces. To overcome this limitation, the paper proposes a neural network-based approach that generates points in areas where geometric information is lacking. The method employs 2D inpainting techniques on color images obtained from the original mesh parameterization and curvature values. The network used in this approach can reconstruct the curvature image, which then serves as a reference for generating a polygonal surface that closely resembles the predicted one. The paper’s experiments show that the proposed method effectively fills complex holes in 3D surfaces with a high degree of naturalness and detail. This paper improves the previous work in terms of a more in-depth explanation of the different stages of the approach as well as an extended results section with exhaustive experiments.},
	journal = {Computers \& Graphics},
	author = {Hernández-Bautista, Marina and Melero, Francisco Javier},
	year = {2023},
	keywords = {Curvature representation, Inpainting, Parameterization, Shape completion},
	pages = {204--215},
}

@article{kocon_chatgpt_2023,
	title = {{ChatGPT}: {Jack} of all trades, master of none},
	volume = {99},
	issn = {1566-2535},
	url = {https://www.sciencedirect.com/science/article/pii/S156625352300177X},
	doi = {https://doi.org/10.1016/j.inffus.2023.101861},
	abstract = {OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and revolutionized the approach in artificial intelligence to human-model interaction. The first contact with the chatbot reveals its ability to provide detailed and precise answers in various areas. Several publications on ChatGPT evaluation test its effectiveness on well-known natural language processing (NLP) tasks. However, the existing studies are mostly non-automated and tested on a very limited scale. In this work, we examined ChatGPT’s capabilities on 25 diverse analytical NLP tasks, most of them subjective even to humans, such as sentiment analysis, emotion recognition, offensiveness, and stance detection. In contrast, the other tasks require more objective reasoning like word sense disambiguation, linguistic acceptability, and question answering. We also evaluated GPT-4 model on five selected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process and analyzed more than 49k responses. Our comparison of its results with available State-of-the-Art (SOTA) solutions showed that the average loss in quality of the ChatGPT model was about 25\% for zero-shot and few-shot evaluation. For GPT-4 model, a loss for semantic tasks is significantly lower than for ChatGPT. We showed that the more difficult the task (lower SOTA performance), the higher the ChatGPT loss. It especially refers to pragmatic NLP problems like emotion recognition. We also tested the ability to personalize ChatGPT responses for selected subjective tasks via Random Contextual Few-Shot Personalization, and we obtained significantly better user-based predictions. Additional qualitative analysis revealed a ChatGPT bias, most likely due to the rules imposed on human trainers by OpenAI. Our results provide the basis for a fundamental discussion of whether the high quality of recent predictive NLP models can indicate a tool’s usefulness to society and how the learning and validation procedures for such systems should be established.},
	journal = {Information Fusion},
	author = {Kocoń, Jan and Cichecki, Igor and Kaszyca, Oliwier and Kochanek, Mateusz and Szydło, Dominika and Baran, Joanna and Bielaniewicz, Julita and Gruza, Marcin and Janz, Arkadiusz and Kanclerz, Kamil and Kocoń, Anna and Koptyra, Bartłomiej and Mieleszczenko-Kowszewicz, Wiktoria and Miłkowski, Piotr and Oleksy, Marcin and Piasecki, Maciej and Radliński, Łukasz and Wojtasik, Konrad and Woźniak, Stanisław and Kazienko, Przemysław},
	year = {2023},
	keywords = {ChatGPT, GPT-4, Large language model, Natural language processing (NLP), Emotion recognition, Humor detection, Model personalization, Natural language inference (NLI), Offensive content, Pragmatic NLP tasks, Prompting, Question answering (QA), Semantic NLP tasks, Sentiment analysis, SOTA analysis, Stance detection, Subjective NLP tasks, Text classification, Word sense disambiguation (WSD)},
	pages = {101861},
}

@article{zhang_ow-tal_2023,
	title = {{OW}-{TAL}: {Learning} {Unknown} {Human} {Activities} for {Open}-{World} {Temporal} {Action} {Localization}},
	volume = {133},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320322005076},
	doi = {https://doi.org/10.1016/j.patcog.2022.109027},
	abstract = {Current temporal action localization methods work well on a closed-world assumption, in which all action categories to be localized are known as a priori. However, this assumption doesn’t apply to open-world scenarios, as novel categories that never appeared in the training stage will be encountered without explicit supervision. Distinct from the closed-world setting, localizing actions under the open-world setup poses two significant challenges: 1) identifying unknown actions from diverse knowns and localizing their temporal boundaries. 2) defying forgetting of previous actions when incrementally updating knowledge of identified unknown actions. To address the aforementioned challenges, we develop a two-branch framework with Unknown and Known action modeling Networks, a.k.a. UK-Net, for the problem of Open-World Temporal Action Localization (OW-TAL). The potential patterns underlying unknown and known actions, as well as their dynamic transformation, are modeled in a unified pipeline. Specifically, a self-attention based position-sensitive module is designed to produce actionness scores for unknown actions in a class-agnostic way. Besides, an iterative optimization strategy is developed to enable knowledge derived from known categories to be shared with the unknowns. In addition, a self-paced learning strategy is proposed to instructionally guide class-incremental learning while defying catastrophic forgetting. Benefiting from the above components, our UK-Net yields superior performance on three challenging datasets, i.e., THUMOS14, ActivityNet1.2, and MUSES. Experimental results also demonstrate the competitive performance of our method when compared with traditional closed-world counterparts.},
	journal = {Pattern Recognition},
	author = {Zhang, Yaru and Zhang, Xiao-Yu and Shi, Haichao},
	year = {2023},
	keywords = {Open-world learning, Self-paced learning, Temporal action localization},
	pages = {109027},
}

@article{scanlon_digital_2023,
	title = {Digital forensic investigation in the age of {ChatGPT}},
	volume = {44},
	issn = {2666-2817},
	url = {https://www.sciencedirect.com/science/article/pii/S2666281723000525},
	doi = {https://doi.org/10.1016/j.fsidi.2023.301543},
	journal = {Forensic Science International: Digital Investigation},
	author = {Scanlon, Mark and Nikkel, Bruce and Geradts, Zeno},
	year = {2023},
	pages = {301543},
}

@article{shi_soft_2023,
	title = {Soft prompt enhanced joint learning for cross-domain aspect-based sentiment analysis},
	volume = {20},
	issn = {2667-3053},
	url = {https://www.sciencedirect.com/science/article/pii/S2667305323001175},
	doi = {https://doi.org/10.1016/j.iswa.2023.200292},
	abstract = {Aspect term extraction is a fundamental task in fine-grained sentiment analysis, aiming to detect customer's opinion targets from reviews about products or services. The traditional supervised models have achieved promising results with annotated datasets. However, their performance dramatically decreases in cross-domain aspect term extraction tasks. Existing cross-domain transfer learning methods face two common limitations: (1) these works directly inject linguistic features into language models, making it challenging to transfer linguistic knowledge to the target domain; (2) they rely on the fixed predefined prompts, which is time-consuming to construct the prompts for all potential aspect term spans. To address the limitations, we propose a soft prompt-based joint learning method for cross-domain aspect term extraction in this paper. Specifically, by incorporating external linguistic features, the proposed method learns domain-invariant representations between source and target domains via multiple objectives, which bridges the gap between domains with varied distributions of aspect terms. Furthermore, the proposed method interpolates a set of transferable soft prompts consisting of multiple learnable vectors that are beneficial to detect aspect terms in the target domain. Extensive experiments are conducted on two groups of datasets and the experimental results demonstrate the effectiveness of the proposed method for cross-domain aspect terms extraction.},
	journal = {Intelligent Systems with Applications},
	author = {Shi, Jingli and Li, Weihua and Bai, Quan and Yang, Yi and Jiang, Jianhua},
	year = {2023},
	keywords = {Aspect-based sentiment analysis, Cross-domain, Soft prompt},
	pages = {200292},
}

@article{fatouros_transforming_2023,
	title = {Transforming sentiment analysis in the financial domain with {ChatGPT}},
	volume = {14},
	issn = {2666-8270},
	url = {https://www.sciencedirect.com/science/article/pii/S2666827023000610},
	doi = {https://doi.org/10.1016/j.mlwa.2023.100508},
	abstract = {Financial sentiment analysis plays a crucial role in decoding market trends and guiding strategic trading decisions. Despite the deployment of advanced deep learning techniques and language models to refine sentiment analysis in finance, this study breaks new ground by investigating the potential of large language models, particularly ChatGPT 3.5, in financial sentiment analysis, with a strong emphasis on the foreign exchange market (forex). Employing a zero-shot prompting approach, we examine multiple ChatGPT prompts on a meticulously curated dataset of forex-related news headlines, measuring performance using metrics such as precision, recall, f1-score, and Mean Absolute Error (MAE) of the sentiment class. Additionally, we probe the correlation between predicted sentiment and market returns as an addition evaluation approach. ChatGPT, compared to FinBERT, a well-established sentiment analysis model for financial texts, exhibited approximately 35\% enhanced performance in sentiment classification and a 36\% higher correlation with market returns. By underlining the significance of prompt engineering, particularly in zero-shot contexts, this study spotlights ChatGPT’s potential to substantially boost sentiment analysis in financial applications. By sharing the utilized dataset, our intention is to stimulate further research and advancements in the field of financial services.},
	journal = {Machine Learning with Applications},
	author = {Fatouros, Georgios and Soldatos, John and Kouroumali, Kalliopi and Makridis, Georgios and Kyriazis, Dimosthenis},
	year = {2023},
	keywords = {ChatGPT, Artificial intelligence, Sentiment analysis, Finance, Risk assessment},
	pages = {100508},
}

@article{yalcinkaya_crystal_2021,
	title = {A crystal plasticity based finite element framework for {RVE} calculations of two-phase materials: {Void} nucleation in dual-phase steels},
	volume = {187},
	issn = {0168-874X},
	url = {https://www.sciencedirect.com/science/article/pii/S0168874X20301906},
	doi = {https://doi.org/10.1016/j.finel.2020.103510},
	abstract = {A crystal plasticity based finite element (CPFE) framework is developed for performing representative volume element (RVE) calculations on two-phase materials. The present paper investigates the mechanical response and the evolution of microstructure of dual-phase (DP) steels under uniaxial tensile loading, with a special focus on void nucleation. The spatial distribution and morphology of the ferrite and martensite grains in DP steels are explicitly accounted for by generating three-dimensional RVEs with Voronoi tessellations. The effects of microstructural parameters—the volume fraction, morphology and spatial distribution of the martensite phase, and the grain size and orientation distribution of the ferrite phase—on the initiation and spread of localized plastic deformation (leading to void nucleation) are investigated in detail. The soft ferrite phase is modelled by a local crystal plasticity theory with anisotropic elasticity, and the hard martensite phase by the J2 flow theory with isotropic elasticity. The developed CPFE framework successfully predicts both the overall and the local mechanical response, and is perfectly capable of distinguishing between different void nucleation mechanisms experimentally observed for DP steels.},
	journal = {Finite Elements in Analysis and Design},
	author = {Yalçinkaya, Tuncay and Çakmak, Serhat Onur and Tekoğlu, Cihan},
	year = {2021},
	keywords = {Dual-phase steels, Finite element method, Localization, Polycrystalline plasticity, Representative volume element},
	pages = {103510},
}

@article{kosonocky_using_2023,
	title = {Using alternative {SMILES} representations to identify novel functional analogues in chemical similarity vector searches},
	issn = {2666-3899},
	url = {https://www.sciencedirect.com/science/article/pii/S2666389923002490},
	doi = {https://doi.org/10.1016/j.patter.2023.100865},
	abstract = {Summary Chemical similarity searches are a widely used family of in silico methods for identifying pharmaceutical leads. These methods historically relied on structure-based comparisons to compute similarity. Here, we use a chemical language model to create a vector-based chemical search. We extend previous implementations by creating a prompt engineering strategy that utilizes two different chemical string representation algorithms: one for the query and the other for the database. We explore this method by reviewing search results from nine queries with diverse targets. We find that the method identifies molecules with similar patent-derived functionality to the query, as determined by our validated LLM-assisted patent summarization pipeline. Further, many of these functionally similar molecules have different structures and scaffolds from the query, making them unlikely to be found with traditional chemical similarity searches. This method may serve as a new tool for the discovery of novel molecular structural classes that achieve target functionality.},
	journal = {Patterns},
	author = {Kosonocky, Clayton W. and Feller, Aaron L. and Wilke, Claus O. and Ellington, Andrew D.},
	year = {2023},
	keywords = {prompt engineering, BERT, canonicalization, chemical similarity search, drug discovery, machine learning, SMILES, transformer, vector search},
	pages = {100865},
}

@article{wang_are_2023,
	title = {Are the {BERT} family zero-shot learners? {A} study on their potential and limitations},
	volume = {322},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370223000991},
	doi = {https://doi.org/10.1016/j.artint.2023.103953},
	abstract = {Starting from the resurgence of deep learning, language models (LMs) have never been so popular. Through simply increasing model scale and data size, large LMs pre-trained with self-supervision objectives demonstrate awe-inspiring results on both task performance and generalization. At the early stage, supervised fine-tuning is indispensable in adapting pre-trained language models (PLMs) to downstream tasks. Later on, the sustained growth of model capacity and data size, as well as newly presented pre-training techniques, make the PLMs perform well under the few-shot setting, especially in the recent paradigm of prompt-based learning. After witnessing the success of PLMs for few-shot tasks, we propose to further study the potential and limitations of PLMs for the zero-shot setting. We utilize 3 models from the most popular BERT family to launch the empirical study on 20 different datasets. We are surprised to find that some simple strategies (without the need of human efforts or unsupervised data) can yield very promising results on a few widely-used datasets, e.g., 88.34\%(±0.60) accuracy on the IMDB dataset, and 84.88\%(±2.83) accuracy on the Amazon dataset, which outperforms manually created prompts without engineering in achieving much better and stable performance with the accuracy of 74.06\%(±13.04), 75.54\%(±11.77) for comparison. However, we also observe some limitations of PLMs under the zero-shot setting, particularly for the language understanding tasks (e.g., GLUE, SuperGLUE).2},
	journal = {Artificial Intelligence},
	author = {Wang, Yue and Wu, Lijun and Li, Juntao and Liang, Xiaobo and Zhang, Min},
	year = {2023},
	keywords = {Prompt-based learning, Pre-trained language model, Zero-shot text classification},
	pages = {103953},
}

@article{bu_efficient_2024,
	title = {Efficient utilization of pre-trained models: {A} review of sentiment analysis via prompt learning},
	volume = {283},
	issn = {0950-7051},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705123008985},
	doi = {https://doi.org/10.1016/j.knosys.2023.111148},
	abstract = {Sentiment analysis is one of the traditional well-known tasks in Natural Language Processing (NLP) research. In recent years, Pre-trained Models (PMs) have become one of the frontiers of NLP, and the knowledge in PMs is usually leveraged to improve machine learning models' performance for a variety of downstream NLP tasks including sentiment analysis. However, there are also some shortcomings in PM-based approaches. For example, many studies pointed out there are gaps between pre-training and fine-tuning. In addition, because of the time-consuming and high-cost data annotation process, the labeled training data are usually precious and scarce, which often leads to the over-fitting of models. The recent advent of prompt learning technology provides a promising solution to the above challenges. In this paper, we first discussed the background of prompt learning and its basic principle. Prompt learning changes the model input by adding templates, allowing learning tasks to adapt actively to pre-trained models, and therefore can promote the innovation and applicability of pre-trained models. Then we investigated the evolution of sentiment analysis and explored the application of prompt learning to different sentiment analysis tasks. Our research and review show that prompt learning is more suitable for sentiment analysis tasks and can achieve good performance. Finally, we also provided some future research directions on prompt-based sentiment analysis. Our survey demonstrated that prompt learning can facilitate the efficient utilization of pre-trained models in sentiment analysis and other tasks, which makes it a new paradigm worthy of further exploration.},
	journal = {Knowledge-Based Systems},
	author = {Bu, Kun and Liu, Yuanchao and Ju, Xiaolong},
	year = {2024},
	keywords = {Natural language processing, Prompt learning, Sentiment analysis, Pre-trained models, Word embedding},
	pages = {111148},
}

@article{espejel_gpt-35_2023,
	title = {{GPT}-3.5, {GPT}-4, or {BARD}? {Evaluating} {LLMs} reasoning ability in zero-shot setting and performance boosting through prompts},
	volume = {5},
	issn = {2949-7191},
	url = {https://www.sciencedirect.com/science/article/pii/S2949719123000298},
	doi = {https://doi.org/10.1016/j.nlp.2023.100032},
	abstract = {Large Language Models (LLMs) have exhibited remarkable performance on various Natural Language Processing (NLP) tasks. However, there is a current hot debate regarding their reasoning capacity. In this paper, we examine the performance of GPT-3.5, GPT-4, and BARD models, by performing a thorough technical evaluation on different reasoning tasks across eleven distinct datasets. Our paper provides empirical evidence showcasing the superior performance of ChatGPT-4 in comparison to both ChatGPT-3.5 and BARD in zero-shot setting throughout almost all evaluated tasks. While the superiority of GPT-4 compared to GPT-3.5 might be explained by its larger size and NLP efficiency, this was not evident for BARD. We also demonstrate that the three models show limited proficiency in Inductive, Mathematical, and Multi-hop Reasoning Tasks. To bolster our findings, we present a detailed and comprehensive analysis of the results from these three models. Furthermore, we propose a set of engineered prompts that enhances the zero-shot setting performance of all three models.},
	journal = {Natural Language Processing Journal},
	author = {Espejel, Jessica López and Ettifouri, El Hassane and Alassan, Mahaman Sanoussi Yahaya and Chouham, El Mehdi and Dahhane, Walid},
	year = {2023},
	keywords = {ChatGPT, Natural Language Processing, BARD, Language models, Reasoning, Zero-shot setting},
	pages = {100032},
}

@article{mazurowski_segment_2023,
	title = {Segment anything model for medical image analysis: {An} experimental study},
	volume = {89},
	issn = {1361-8415},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841523001780},
	doi = {https://doi.org/10.1016/j.media.2023.102918},
	abstract = {Training segmentation models for medical images continues to be challenging due to the limited availability of data annotations. Segment Anything Model (SAM) is a foundation model trained on over 1 billion annotations, predominantly for natural images, that is intended to segment user-defined objects of interest in an interactive manner. While the model performance on natural images is impressive, medical image domains pose their own set of challenges. Here, we perform an extensive evaluation of SAM’s ability to segment medical images on a collection of 19 medical imaging datasets from various modalities and anatomies. In our experiments, we generated point and box prompts for SAM using a standard method that simulates interactive segmentation. We report the following findings: (1) SAM’s performance based on single prompts highly varies depending on the dataset and the task, from IoU=0.1135 for spine MRI to IoU=0.8650 for hip X-ray. (2) Segmentation performance appears to be better for well-circumscribed objects with prompts with less ambiguity such as the segmentation of organs in computed tomography and poorer in various other scenarios such as the segmentation of brain tumors. (3) SAM performs notably better with box prompts than with point prompts. (4) SAM outperforms similar methods RITM, SimpleClick, and FocalClick in almost all single-point prompt settings. (5) When multiple-point prompts are provided iteratively, SAM’s performance generally improves only slightly while other methods’ performance improves to the level that surpasses SAM’s point-based performance. We also provide several illustrations for SAM’s performance on all tested datasets, iterative segmentation, and SAM’s behavior given prompt ambiguity. We conclude that SAM shows impressive zero-shot segmentation performance for certain medical imaging datasets, but moderate to poor performance for others. SAM has the potential to make a significant impact in automated medical image segmentation in medical imaging, but appropriate care needs to be applied when using it. Code for evaluation SAM is made publicly available at https://github.com/mazurowski-lab/segment-anything-medical-evaluation.},
	journal = {Medical Image Analysis},
	author = {Mazurowski, Maciej A. and Dong, Haoyu and Gu, Hanxue and Yang, Jichen and Konz, Nicholas and Zhang, Yixin},
	year = {2023},
	keywords = {Foundation models, Deep learning, Segmentation},
	pages = {102918},
}

@article{martins_combining_2023,
	title = {Combining low-code development with {ChatGPT} to novel no-code approaches: {A} focus-group study},
	volume = {20},
	issn = {2667-3053},
	url = {https://www.sciencedirect.com/science/article/pii/S266730532300114X},
	doi = {https://doi.org/10.1016/j.iswa.2023.200289},
	abstract = {Low-code tools are a trend in software development for business solutions due to their agility and ease of use. There are a certain number of vendors with such solutions. Still, in most Western countries, there is a clear need for the existence of greater quantities of certified and experienced professionals to work with those tools. This means that companies with more resources can attract and maintain those professionals, whilst other smaller organizations must rely on an endless search for this scarce resource. We will present and validate a model designed to transform ChatGPT into a low-code developer, addressing the demand for a more skilled human resource solution. This innovative tool underwent rigorous validation via a focus group study, engaging a panel of highly experienced experts. Their invaluable insights and feedback on the proposed model were systematically gathered and meticulously analysed.},
	journal = {Intelligent Systems with Applications},
	author = {Martins, José and Branco, Frederico and Mamede, Henrique},
	year = {2023},
	keywords = {ChatGPT, LLM, Artificial intelligence, Low-code, No-code, Software models},
	pages = {200289},
}

@article{veres_self_2023,
	title = {Self supervised learning and the poverty of the stimulus},
	volume = {147},
	issn = {0169-023X},
	url = {https://www.sciencedirect.com/science/article/pii/S0169023X2300068X},
	doi = {https://doi.org/10.1016/j.datak.2023.102208},
	abstract = {Diathesis alternations are the possible expressions of the arguments of verbs in different, systematically related subcategorization frames. Semantically similar verbs such as spill and spray can behave differently with respect to the alternations they can participate in. For example one can “spill/spray water on the plant”, but while one can “spray the plant with water”, it is odd to say “spill the plant with water”. “Spray” is a verb which can alternate between syntactic frames while “spill” is not alternating. How human speakers learn the difference between such verbs is not clearly understood, because the primary linguistic data (PLD) they receive does not appear sufficient to infer the knowledge required for adult competence. More generally the poverty of the stimulus (POS) hypothesis states that the PLD is not sufficient for a learner to infer full adult competence of language. That is, learning relies on prior constraints introduced by the language faculty. We tested state-of-the-art machine learning models trained by self supervision, and found some evidence that they could in fact learn the correct pattern of acceptability judgement in the locative alternation. However, we argued that this was partially a result of fine-tuning which introduced negative evidence into the learning data, which facilitated shortcut learning. Large language models (LLMs) cannot learn some linguistic facts from normal language data, but they can compensate to some extent by learning spurious correlated features when negative feedback is introduced during the training cycle.},
	journal = {Data \& Knowledge Engineering},
	author = {Veres, Csaba and Sampson, Jennifer},
	year = {2023},
	keywords = {NLP, Classification, Language, Learnability, Machine Learning, Text mining},
	pages = {102208},
}

@article{zhao_prompt_2023,
	title = {Prompt learning for metonymy resolution: {Enhancing} performance with internal prior knowledge of pre-trained language models},
	volume = {279},
	issn = {0950-7051},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705123006780},
	doi = {https://doi.org/10.1016/j.knosys.2023.110928},
	abstract = {Linguistic metonymy is a common type of figurative language in natural language processing (NLP), where a concept is represented by a closely associated word or phrase, for example “business executives suits”. As a result, metonymy resolution has become an important NLP task aimed at correctly identifying metonymic expressions within sentences. Previous approaches to this task have typically relied on pre-trained language models (PLMs) using a fine-tuning process. However, this can be time-consuming and resource-intensive, and may lead to a loss of factual prior knowledge. The emergence of a novel learning paradigm termed “prompt learning” or “prompt-tuning” has recently sparked widespread interest and captured considerable attention, as it has proven to yield remarkable results and surpass previous benchmarks. This approach uses a “pre-train→prompt→predict” paradigm and has been shown to better utilize the internal prior knowledge of a PLM, especially in situations with limited supervised resources. Inspired by this success, we investigated how prompt learning could improve metonymy resolution. We have developed a series of prompt learning approaches, called PromptMR, for metonymy resolution, and applied them to several widely-used metonymy resolution datasets. We also designed additional prompt-tuning augmentation strategies to further enhance the potential of prompt learning. Our experiments demonstrated that our method achieved state-of-the-art performance over multiple competitive baselines in both data-sufficient and data-scarce scenarios. The code implementations for PromptMR are accessible on GitHub via the URL: https://github.com/albert-jin/PromptTuning2MetonymyResolution.},
	journal = {Knowledge-Based Systems},
	author = {Zhao, Biao and Jin, Weiqiang and Zhang, Yu and Huang, Subin and Yang, Guang},
	year = {2023},
	keywords = {Prompt learning, Pre-trained language model, Answer engineering, Metonymy resolution, Prompting template engineering},
	pages = {110928},
}

@article{chari_informing_2023,
	title = {Informing clinical assessment by contextualizing post-hoc explanations of risk prediction models in type-2 diabetes},
	volume = {137},
	issn = {0933-3657},
	url = {https://www.sciencedirect.com/science/article/pii/S093336572300012X},
	doi = {https://doi.org/10.1016/j.artmed.2023.102498},
	abstract = {Medical experts may use Artificial Intelligence (AI) systems with greater trust if these are supported by ‘contextual explanations’ that let the practitioner connect system inferences to their context of use. However, their importance in improving model usage and understanding has not been extensively studied. Hence, we consider a comorbidity risk prediction scenario and focus on contexts regarding the patients’ clinical state, AI predictions about their risk of complications, and algorithmic explanations supporting the predictions. We explore how relevant information for such dimensions can be extracted from Medical guidelines to answer typical questions from clinical practitioners. We identify this as a question answering (QA) task and employ several state-of-the-art Large Language Models (LLM) to present contexts around risk prediction model inferences and evaluate their acceptability. Finally, we study the benefits of contextual explanations by building an end-to-end AI pipeline including data cohorting, AI risk modeling, post-hoc model explanations, and prototyped a visual dashboard to present the combined insights from different context dimensions and data sources, while predicting and identifying the drivers of risk of Chronic Kidney Disease (CKD) - a common type-2 diabetes (T2DM) comorbidity. All of these steps were performed in deep engagement with medical experts, including a final evaluation of the dashboard results by an expert medical panel. We show that LLMs, in particular BERT and SciBERT, can be readily deployed to extract some relevant explanations to support clinical usage. To understand the value-add of the contextual explanations, the expert panel evaluated these regarding actionable insights in the relevant clinical setting. Overall, our paper is one of the first end-to-end analyses identifying the feasibility and benefits of contextual explanations in a real-world clinical use case. Our findings can help improve clinicians’ usage of AI models.},
	journal = {Artificial Intelligence in Medicine},
	author = {Chari, Shruthi and Acharya, Prasant and Gruen, Daniel M. and Zhang, Olivia and Eyigoz, Elif K. and Ghalwash, Mohamed and Seneviratne, Oshani and Saiz, Fernando Suarez and Meyer, Pablo and Chakraborty, Prithwish and McGuinness, Deborah L.},
	year = {2023},
	keywords = {Clinical explainability, Contextual explanations, Question-answering approach, Type-2 diabetes comorbidity risk prediction, User-driven},
	pages = {102498},
}

@inproceedings{lee_chatgpt-based_2023,
	title = {{ChatGPT}-{Based} {Debate} {Game} {Application} {Utilizing} {Prompt} {Engineering}},
	isbn = {979-840070228-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174290777&doi=10.1145%2f3599957.3606244&partnerID=40&md5=34cfa87795f81cd3aaeddff1423eb9a7},
	doi = {10.1145/3599957.3606244},
	abstract = {This paper1 focuses on the implementation of a debate game using ChatGPT, aiming to investigate the feasibility of incorporating large language models into the educational domain through prompt engineering. The study explores strategies to elicit desired outputs from the GPT model by employing the prompt engineering methodology, as provided by Microsoft. Specifically, the game implementation involves the customization of ChatGPT's responses to facilitate a natural progression of debates, varying levels of difficulty, and an evaluation system for assessing the quality of discourse. By leveraging the prompt engineering methodology, we demonstrate that providing specific instructions or case-based prompts improves the accuracy and relevance of ChatGPT's answers. The developed application targets teenagers, enabling them to engage in real-time debates with ChatGPT and enhance their literacy skills. Furthermore, the game fosters the development of logical reasoning, persuasive abilities, effective expression, active participation, and attentive listening while expressing personal opinions, ultimately fostering a sense of accomplishment. Moreover, through debate evaluation and personalized advice, ChatGPT is expected to recognize and address its shortcomings, thereby continuously improving its conversational capabilities. Overall, this research contributes to the understanding of how large language models can be harnessed in educational settings and underscores the potential benefits of prompt engineering techniques in optimizing the outputs of such models. © 2023 ACM.},
	language = {English},
	booktitle = {2023 {Research} in {Adaptive} and {Convergent} {Systems} {RACS} 2023},
	publisher = {Association for Computing Machinery, Inc},
	author = {Lee, Eun-Young and Il, Ngagaba Gogo Dae and An, Gi-Hong and Lee, Sungchul and Lim, Kiho},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {ChatGPT, Prompt engineering, Large language model, Language model, Case based, Computational linguistics, Customisation, Engineering education, Engineering methodology, Level of difficulties, MicroSoft, Quality control, Specific instruction},
	annote = {Cited by: 0; Conference name: 2023 Research in Adaptive and Convergent Systems, RACS 2023; Conference date: 6 August 2023 through 10 August 2023; Conference code: 192554},
}

@article{voetman_using_2023,
	title = {Using {Diffusion} {Models} for {Dataset} {Generation}: {Prompt} {Engineering} vs. {Fine}-{Tuning}},
	volume = {14184 LNCS},
	issn = {03029743},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174442198&doi=10.1007%2f978-3-031-44237-7_14&partnerID=40&md5=af9b52bbe65833dd3ab8869624108929},
	doi = {10.1007/978-3-031-44237-7_14},
	abstract = {Despite the notable achievements of deep object detection models, a major challenge remains to be the need for vast amounts of training data. The process of acquiring such real-world data is laborious, prompting the exploration of new research directions such as synthetic data generation. In this study, we assess the capability of two distinct synthetic data generating techniques utilising stable diffusion, namely, (1) Prompt engineering of an established model and (2) Fine-tuning a pretrained model. As a result, we generate two training datasets, manually annotate them, and train separate object detection models for testing on a real-world detection dataset. The results demonstrate that both prompt engineering and fine-tuning exhibit similar performance when tested on a set of 331 real-world images, in the context of apple detection in apple orchards. We compared their performance with the baseline setting where the model was trained on real-world images and witnessed only a 0.07 and 0.08 average precision deviation from the baseline model. Qualitative results demonstrate that both models are able to accurately predict the location of the apples, except in instances of heavy shading. This study distinguishes itself from prior research by focusing on object detection instead of image classification. Furthermore, we are the first to apply diffusion model fine-tuning in the context of dataset generation. Our findings underscore the potential of synthetic data generation as a viable alternative to the laborious collection of extensive training data for object detection models. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.},
	language = {English},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Voetman, Roy and van Meekeren, Alexander and Aghaei, Maya and Dijkstra, Klaas},
	editor = {N, Tsapatsoulis and E, Kyriacou and A, Lanitis and Z, Theodosiou and M, Pattichis and C, Pattichis and C, Kyrkou and A, Panayides},
	year = {2023},
	note = {ISBN: 978-303144236-0
Publisher: Springer Science and Business Media Deutschland GmbH
Type: Conference paper},
	keywords = {Prompt engineering, Detection, Detection models, Diffusion, Diffusion model, Dreambooth, Fine tuning, Fruits, Object detection, Object recognition, Objects detection, Real-world, Stable diffusion, Statistical tests, Training data},
	pages = {142 -- 152},
	annote = {Cited by: 0; Conference name: 20th International Conference on Computer Analysis of Images and Patterns, CAIP 2023; Conference date: 25 September 2023 through 28 September 2023; Conference code: 301519},
}

@inproceedings{liu_design_2022,
	title = {Design {Guidelines} for {Prompt} {Engineering} {Text}-to-{Image} {Generative} {Models}},
	isbn = {978-1-4503-9157-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130510763&doi=10.1145%2f3491102.3501825&partnerID=40&md5=7fc1a4487fcde557808a44344a62ef2f},
	doi = {10.1145/3491102.3501825},
	abstract = {Text-to-image generative models are a new and powerful way to generate visual artwork. However, the open-ended nature of text as interaction is double-edged; while users can input anything and have access to an infinite range of generations, they also must engage in brute-force trial and error with the text prompt when the result quality is poor. We conduct a study exploring what prompt keywords and model hyperparameters can help produce coherent outputs. In particular, we study prompts structured to include subject and style keywords and investigate success and failure modes of these prompts. Our evaluation of 5493 generations over the course of five experiments spans 51 abstract and concrete subjects as well as 51 abstract and figurative styles. From this evaluation, we present design guidelines that can help people produce better outcomes from text-to-image generative models. © 2022 ACM.},
	language = {English},
	booktitle = {Conference on {Human} {Factors} in {Computing} {Systems} - {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Vivian and Chilton, Lydia B},
	year = {2022},
	note = {Type: Conference paper},
	keywords = {Prompt engineering, Multi-modal, Abstracting, AI co-creation, Brute force, Co-creation, Computational creativities, Design, Design guideline, Generative model, Multimodal generative model, Petroleum reservoir evaluation, Text-to-image},
	annote = {Cited by: 53; Conference name: 2022 CHI Conference on Human Factors in Computing Systems, CHI 2022; Conference date: 30 April 2022 through 5 May 2022; Conference code: 179031; All Open Access, Green Open Access},
}

@inproceedings{ruskov_grimm_2023,
	title = {Grimm in {Wonderland}: {Prompt} {Engineering} with {Midjourney} to {Illustrate} {Fairytales}},
	volume = {3365},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152035311&partnerID=40&md5=6f1a6849312da48dd73b0de31d385c7d},
	abstract = {The quality of text-to-image generation is continuously improving, yet the boundaries of its applicability are still unclear. In particular, refinement of the text input with the objective of achieving better results – commonly called prompt engineering – so far seems to have not been geared towards work with preexisting texts. We investigate whether text-to-image generation and prompt engineering could be used to generate basic illustrations of popular fairytales. Using Midjourney v4, we engage in action research with a dual aim: to attempt to generate 5 believable illustrations for each of 5 popular fairytales, and to define a prompt engineering process that starts from a pre-existing text and arrives at an illustration of it. We arrive at a tentative 4-stage process: i) initial prompt, ii) composition adjustment, iii) style refinement, and iv) variation selection. We also discuss three reasons why the generation model struggles with certain illustrations: difficulties with counts, bias from stereotypical configurations and inability to depict overly fantastic situations. Our findings are not limited to the specific generation model and are intended to be generalisable to future ones. © 2023 Copyright for this paper by its authors.},
	language = {English},
	booktitle = {{CEUR} {Workshop} {Proceedings}},
	publisher = {CEUR-WS},
	author = {Ruskov, Martin},
	editor = {A, Falcon and S, Ferilli and A, Bardi and S, Marchesin and D, Redavid},
	year = {2023},
	note = {ISSN: 16130073
Type: Conference paper},
	keywords = {Prompt engineering, Image enhancement, Action research, Engineering process, Fairytale, Image generations, Text input, Text-to-image generation},
	pages = {180 -- 191},
	annote = {Cited by: 0; Conference name: 19th Conference on Information and Research Science Connecting to Digital and Library Science, IRCDL 2023; Conference date: 23 February 2023 through 24 February 2023; Conference code: 187542},
}

@article{cain_prompting_2023,
	title = {Prompting {Change}: {Exploring} {Prompt} {Engineering} in {Large} {Language} {Model} {AI} and {Its} {Potential} to {Transform} {Education}},
	issn = {87563894},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174411230&doi=10.1007%2fs11528-023-00896-0&partnerID=40&md5=2f782f6b6e182aef07f17121ada49334},
	doi = {10.1007/s11528-023-00896-0},
	abstract = {This paper explores the transformative potential of Large Language Models Artificial Intelligence (LLM AI) in educational contexts, particularly focusing on the innovative practice of prompt engineering. Prompt engineering, characterized by three essential components of content knowledge, critical thinking, and iterative design, emerges as a key mechanism to access the transformative capabilities of LLM AI in the learning process. This paper charts the evolving trajectory of LLM AI as a tool poised to reshape educational practices and assumptions. In particular, this paper breaks down the potential of prompt engineering practices to enhance learning by fostering personalized, engaging, and equitable educational experiences. The paper underscores how the natural language capabilities of LLM AI tools can help students and educators transition from passive recipients to active co-creators of their learning experiences. Critical thinking skills, particularly information literacy, media literacy, and digital citizenship, are identified as crucial for using LLM AI tools effectively and responsibly. Looking forward, the paper advocates for continued research to validate the benefits of prompt engineering practices across diverse learning contexts while simultaneously promoting potential defects, biases, and ethical concerns related to LLM AI use in education. It calls upon practitioners to explore and train educational stakeholders in best practices around prompt engineering for LLM AI, fostering progress towards a more engaging and equitable educational future. © 2023, Association for Educational Communications \& Technology.},
	language = {English},
	journal = {TechTrends},
	author = {Cain, William},
	year = {2023},
	note = {Publisher: Springer
Type: Article},
	annote = {Cited by: 0},
}

@article{schacht_promptie_2023,
	title = {{PromptIE} - {Information} {Extraction} with {Prompt}-{Engineering} and {Large} {Language} {Models}},
	volume = {1836 CCIS},
	issn = {18650929},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169446635&doi=10.1007%2f978-3-031-36004-6_69&partnerID=40&md5=aaa87fd7da4abe0e61440c2e6b0679b3},
	doi = {10.1007/978-3-031-36004-6_69},
	abstract = {Extracting triples of subjects, objects, and predicates from text to populate knowledge bases traditionally involves several intermediate steps such as co-reference resolution, named entity recognition, and relationship extraction. Treating triple extraction as translation task from source sentences to sets of triples, we present an end-to-end solution for information extraction that uses task prefixes to prompts a fine-tuned large language model to extract triples from text. Thus, the need for data labeling and training multiple models is reduced. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	language = {English},
	journal = {Communications in Computer and Information Science},
	author = {Schacht, Sigurd and Kamath Barkur, Sudarshan and Lanquillon, Carsten},
	editor = {C, Stephanidis and M, Antona and S, Ntoa and G, Salvendy},
	year = {2023},
	note = {ISBN: 978-303136003-9
Publisher: Springer Science and Business Media Deutschland GmbH
Type: Conference paper},
	keywords = {Prompt engineering, Large language model, Artificial intelligence, Language model, Computational linguistics, Character recognition, Coreference resolution, Data mining, End-to-end solutions, Entity-relationship, Information retrieval, Named entity recognition, Openie, Relationship extraction, Triple extraction},
	pages = {507 -- 514},
	annote = {Cited by: 0; Conference name: 25th International Conference on Human-Computer Interaction, HCII 2023; Conference date: 23 July 2023 through 28 July 2023; Conference code: 297769},
}

@article{busch_just_2023,
	title = {Just {Tell} {Me}: {Prompt} {Engineering} in {Business} {Process} {Management}},
	volume = {479 LNBIP},
	issn = {18651348},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166157715&doi=10.1007%2f978-3-031-34241-7_1&partnerID=40&md5=c26596bcb64bbebba577c47e52226828},
	doi = {10.1007/978-3-031-34241-7_1},
	abstract = {GPT-3 and several other language models (LMs) can effectively address various natural language processing (NLP) tasks, including machine translation and text summarization. Recently, they have also been successfully employed in the business process management (BPM) domain, e.g., for predictive process monitoring and process extraction from text. This, however, typically requires fine-tuning the employed LM, which, among others, necessitates large amounts of suitable training data. A possible solution to this problem is the use of prompt engineering, which leverages pre-trained LMs without fine-tuning them. Recognizing this, we argue that prompt engineering can help bring the capabilities of LMs to BPM research. We use this position paper to develop a research agenda for the use of prompt engineering for BPM research by identifying the associated potentials and challenges. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	language = {English},
	journal = {Lecture Notes in Business Information Processing},
	author = {Busch, Kiran and Rochlitzer, Alexander and Sola, Diana and Leopold, Henrik},
	editor = {H, van der Aa and D, Bork and H.A, Proper and R, Schmidt},
	year = {2023},
	note = {ISBN: 978-303134240-0
Publisher: Springer Science and Business Media Deutschland GmbH
Type: Conference paper},
	keywords = {Language model, Natural language processing systems, Administrative data processing, Business Process, Enterprise resource management, Language processing, Machine translations, Management domains, Management research, Natural languages, Predictive process, Process management, Process monitoring, Text Summarisation},
	pages = {3 -- 11},
	annote = {Cited by: 0; Conference name: 24th International Conference on Business Process Modeling, Development, and Support, BPMDS 2023 and 28th International Conference on Exploring Modeling Methods for Systems Analysis and Development, EMMSAD 2023; Conference date: 12 June 2023 through 13 June 2023; Conference code: 295839; All Open Access, Green Open Access},
}

@inproceedings{denny_conversing_2023,
	title = {Conversing with {Copilot}: {Exploring} {Prompt} {Engineering} for {Solving} {CS1} {Problems} {Using} {Natural} {Language}},
	volume = {1},
	isbn = {978-1-4503-9431-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149847393&doi=10.1145%2f3545945.3569823&partnerID=40&md5=b582bfd620b7d6f37ccc4d5d554a7d94},
	doi = {10.1145/3545945.3569823},
	abstract = {GitHub Copilot is an artificial intelligence tool for automatically generating source code from natural language problem descriptions. Since June 2022, Copilot has officially been available for free to all students as a plug-in to development environments like Visual Studio Code. Prior work exploring OpenAI Codex, the underlying model that powers Copilot, has shown it performs well on typical CS1 problems thus raising concerns about its potential impact on how introductory programming courses are taught. However, little is known about the types of problems for which Copilot does not perform well, or about the natural language interactions that a student might have with Copilot when resolving errors. We explore these questions by evaluating the performance of Copilot on a publicly available dataset of 166 programming problems. We find that it successfully solves around half of these problems on its very first attempt, and that it solves 60\% of the remaining problems using only natural language changes to the problem description. We argue that this type of prompt engineering, which we believe will become a standard interaction between human and Copilot when it initially fails, is a potentially useful learning activity that promotes computational thinking skills, and is likely to change the nature of code writing skill development. © 2023 ACM.},
	language = {English},
	booktitle = {{SIGCSE} 2023 - {Proceedings} of the 54th {ACM} {Technical} {Symposium} on {Computer} {Science} {Education}},
	publisher = {Association for Computing Machinery, Inc},
	author = {Denny, Paul and Kumar, Viraj and Giacaman, Nasser},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Large language model, Foundation models, Artificial intelligence, Language model, Engineering education, Natural languages, Artificial intelligence tools, Cs1, Education computing, Github copilot, Introductory programming, Openai, Problem description, Students},
	pages = {1136 -- 1142},
	annote = {Cited by: 13; Conference name: 54th ACM Technical Symposium on Computer Science Education, SIGCSE 2023; Conference date: 15 March 2023 through 18 March 2023; Conference code: 187022; All Open Access, Green Open Access},
}

@inproceedings{lee_improving_2023,
	title = {Improving {Formality}-{Sensitive} {Machine} {Translation} using {Data}-{Centric} {Approaches} and {Prompt} {Engineering}},
	isbn = {978-1-959429-84-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172435087&partnerID=40&md5=c8e193a425a4758029f38b0e9a6f6623},
	abstract = {In this paper, we present the KU x Upstage team’s submission for the Special Task on Formality Control on Spoken Language Translation, which involves translating English into four languages with diverse grammatical formality markers. Our methodology comprises two primary components: 1) a language-specific data-driven approach, and 2) the generation of synthetic data through the employment of large-scale language models and empirically-grounded prompt engineering. By adapting methodologies and models to accommodate the unique linguistic properties of each language, we observe a notable enhancement in performance relative to the baseline, substantiating the heightened efficacy of data-driven approaches. Moreover, our devised prompt engineering strategy yields superior synthetic translation instances. © IWSLT 2023.All rights reserved.},
	language = {English},
	booktitle = {20th {International} {Conference} on {Spoken} {Language} {Translation}, {IWSLT} 2023 - {Proceedings} of the {Conference}},
	publisher = {Association for Computational Linguistics},
	author = {Lee, Seungjun and Moon, Hyeonseok and Park, Chanjun and Lim, Heuiseok},
	editor = {E, Salesky and M, Federico and M, Carpuat},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Language model, Performance, Computational linguistics, Machine translations, Computer aided language translation, Data-centric approaches, Data-driven approach, Large-scales, Linguistic properties, Machine translation, Spoken language translation, Synthetic data},
	pages = {420 -- 432},
	annote = {Cited by: 1; Conference name: 20th International Conference on Spoken Language Translation, IWSLT 2023; Conference date: 13 July 2023 through 14 July 2023; Conference code: 192856},
}

@article{henrickson_prompting_2023,
	title = {Prompting meaning: a hermeneutic approach to optimising prompt engineering with {ChatGPT}},
	issn = {09515666},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169786535&doi=10.1007%2fs00146-023-01752-8&partnerID=40&md5=4fd95fff6bb9d352e8877073cadc5877},
	doi = {10.1007/s00146-023-01752-8},
	abstract = {Recent advances in natural language generation (NLG), such as public accessibility to ChatGPT, have sparked polarised debates about the societal impact of this technology. Popular discourse tends towards either overoptimistic hype that touts the radically transformative potentials of these systems or pessimistic critique of their technical limitations and general ‘stupidity’. Surprisingly, these debates have largely overlooked the exegetical capacities of these systems, which for many users seem to be producing meaningful texts. In this paper, we take an interdisciplinary approach that combines hermeneutics—the study of meaning and interpretation—with prompt engineering—task descriptions embedded in input to NLG systems—to study the extent to which a specific NLG system, ChatGPT, produces texts of hermeneutic value. We design prompts with the goal of optimising hermeneuticity rather than mere factual accuracy, and apply them in four different use cases combining humans and ChatGPT as readers and writers. In most cases, ChatGPT produces readable texts that respond clearly to our requests. However, increasing the specificity of prompts’ task descriptions leads to texts with intensified neutrality, indicating that ChatGPT’s optimisation for factual accuracy may actually be detrimental to the hermeneuticity of its output. © 2023, The Author(s).},
	language = {English},
	journal = {AI and Society},
	author = {Henrickson, Leah and Meroño-Peñuela, Albert},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Article},
	keywords = {ChatGPT, Natural language processing, Prompt engineering, Large language model, Language model, Natural language processing systems, Language processing, Natural languages, Embedded systems, Hermeneutic, Natural language generation, Task description},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@article{clavie_large_2023,
	title = {Large {Language} {Models} in the {Workplace}: {A} {Case} {Study} on {Prompt} {Engineering} for {Job} {Type} {Classification}},
	volume = {13913 LNCS},
	issn = {03029743},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164666382&doi=10.1007%2f978-3-031-35320-8_1&partnerID=40&md5=bc49eebfb1c219eae23adc9e62bc333f},
	doi = {10.1007/978-3-031-35320-8_1},
	abstract = {This case study investigates the task of job classification in a real-world setting, where the goal is to determine whether an English-language is appropriate for a graduate or entry-level position. We explore multiple approaches to text classification, including supervised approaches such as traditional models like Support Vector Machines (SVMs) and state-of-the-art deep learning methods such as DeBERTa. We compare them with Large Language Models (LLMs) used in both few-shot and zero-shot classification settings. To accomplish this task, we employ prompt engineering, a technique that involves designing prompts to guide the LLMs towards the desired output. Specifically, we evaluate the performance of two commercially available state-of-the-art GPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We also conduct a detailed analysis of the impact of different aspects of prompt engineering on the model’s performance. Our results show that, with a well-designed prompt, a zero-shot gpt-3.5-turboclassifier outperforms all other models, achieving a 6\% increase in Precision@95\% Recall compared to the best supervised approach. Furthermore, we observe that the wording of the prompt is a critical factor in eliciting the appropriate “reasoning” in the model, and that seemingly minor aspects of the prompt significantly affect the model’s performance. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	language = {English},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Clavié, Benjamin and Ciceu, Alexandru and Naylor, Frederick and Soulié, Guillaume and Brightwell, Thomas},
	editor = {E, Métais and F, Meziane and W, Manning and S, Reiff-Marganiec and V, Sugumaran},
	year = {2023},
	note = {ISBN: 978-303135319-2
Publisher: Springer Science and Business Media Deutschland GmbH
Type: Conference paper},
	keywords = {Natural language processing, Prompt engineering, Large language model, Deep learning, Language model, Text classification, Learning systems, Performance, Zero-shot learning, Computational linguistics, Natural language processing systems, Language processing, Natural languages, Case-studies, Classification (of information), State of the art, Support vector machines, Text processing},
	pages = {3 -- 17},
	annote = {Cited by: 3; Conference name: 28th International Conference on Applications of Natural Language to Information Systems, NLDB 2023; Conference date: 21 June 2023 through 23 June 2023; Conference code: 296599; All Open Access, Green Open Access},
}

@article{ali_supporting_2023,
	title = {Supporting self-directed learning and self-assessment using {TeacherGAIA}, a generative {AI} chatbot application: {Learning} approaches and prompt engineering},
	volume = {9},
	issn = {23735082},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172808547&doi=10.1080%2f23735082.2023.2258886&partnerID=40&md5=96d8b6e15284ca983bf1993d3b1c30ca},
	doi = {10.1080/23735082.2023.2258886},
	abstract = {Self-directed learning and self-assessment require student responsibility over learning needs, goals, processes, and outcomes. However, this student-led learning can be challenging to achieve in a classroom limited by a one-to-many teacher-led instruction. We, thus, have designed and prototyped a generative artificial intelligence chatbot application (GAIA), named TeacherGAIA, that can be used to asynchronously support students in their self-directed learning and self-assessment outside the classroom. We first identified diverse constructivist learning approaches that align with, and promote, student-led learning. These included knowledge construction, inquiry-based learning, self-assessment, and peer teaching. The in-context learning abilities of large language model (LLM) from OpenAI were then leveraged via prompt engineering to steer interactions supporting these different learning approaches. These interactions contrasted with ChatGPT, OpenAI’s chatbot which by default engaged in the traditional transmissionist mode of learning reminiscent of teacher-led instruction. Preliminary design, prompt engineering and prototyping suggested fidelity to the learning approaches, cognitive guidance, and social-emotional support, all of which were implemented in a generative AI manner without pre-specified rules or “hard-coding”. Other affordances of TeacherGAIA are discussed and future development outlined. We anticipate TeacherGAIA to be a useful application for teachers in facilitating self-directed learning and self-assessment among K-12 students. © 2023 Informa UK Limited, trading as Taylor \& Francis Group.},
	language = {English},
	number = {2},
	journal = {Learning: Research and Practice},
	author = {Ali, Farhan and Choy, Doris and Divaharan, Shanti and Tay, Hui Yong and Chen, Wenli},
	year = {2023},
	note = {Publisher: Routledge
Type: Article},
	pages = {135 -- 147},
	annote = {Cited by: 1},
}

@inproceedings{zhang_ds4dh_2023,
	title = {{DS4DH} at {MEDIQA}-{Chat} 2023: {Leveraging} {SVM} and {GPT}-3 {Prompt} {Engineering} for {Medical} {Dialogue} {Classification} and {Summarization}},
	isbn = {978-1-959429-88-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175443584&partnerID=40&md5=8832b90c2ca30f9a12c3df48f053f87a},
	abstract = {This paper presents the results of the Data Science for Digital Health (DS4DH) group in the MEDIQA-Chat Tasks at ACL-ClinicalNLP 2023. Our study combines the power of a classical machine learning method, Support Vector Machine, for classifying medical dialogues, along with the implementation of oneshot prompts using GPT-3.5. We employ dialogues and summaries from the same category as prompts to generate summaries for novel dialogues. Our findings exceed the average benchmark score, offering a robust reference for assessing performance in this field. © 2023 Association for Computational Linguistics.},
	language = {English},
	booktitle = {Proceedings of the {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics (ACL)},
	author = {Zhang, Boya and Mishra, Rahul and Teodoro, Douglas},
	year = {2023},
	note = {ISSN: 0736587X
Type: Conference paper},
	keywords = {Learning systems, Performance, Computational linguistics, Natural language processing systems, Classification (of information), Support vector machines, Benchmarking, Machine learning methods, Power, Support vectors machine},
	pages = {536 -- 545},
	annote = {Cited by: 0; Conference name: 5th Workshop on Clinical Natural Language Processing, ClinicalNLP 2023. held at ACL 2023; Conference date: 14 July 2023; Conference code: 193206},
}

@inproceedings{sun_integrate_2022,
	title = {{INTEGRATE} {ONLINE} {QUERY}\&{DEFENSE} {INTO} {PROJECT} {BASED} {LEARNING} {TO} {PROMPT} {ENGINEERING} {AUTHENTIC} {EXPERIENCES}},
	volume = {10},
	isbn = {978-1-71387-116-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159726173&partnerID=40&md5=ff7e1045eb9d53a5a9613ab0b3d2b41f},
	abstract = {As an effective student-centered pedagogical approach in improving students' academic knowledge, teamwork skills, communication skills and leadership, Project Based Learning (PBL) has been widely adopted in engineering courses delivering during the last decades. However, there is a challenge for both the teachers and the students during the PBL: Most project results are fragile to their operation environment because of the students' poor understanding of the engineering authentic problems and integrity requirements or even neglect these, while engineering complexity and functional integrity is essential characteristic in aviation and other safety related fields, engineering authenticity and integrity analyzation ability and habit are basic requirements for future aviation engineers. Taking navigation principles and systems course PBL as a case, use online query \& defense challenge competition to promote the students to conduct in-depth discussion and analysis on the engineering authenticity and integrity requirements in the early stage of project implementation, so as to improve the students' project participation and outcomes. Project outcomes are assessed and compared with no query \& defense activities integrated ones, data are collected and analyzed, the test results show that the online query \& defense pedagogical form can effectively help the students to improve their engineering authenticity and integrity thinking. © (2022) by International Council of Aeronautical Sciences (ICAS) All rights reserved.},
	language = {English},
	booktitle = {33rd {Congress} of the {International} {Council} of the {Aeronautical} {Sciences}, {ICAS} 2022},
	publisher = {International Council of the Aeronautical Sciences},
	author = {Sun, Shuguang and Gong, Ruoxin},
	year = {2022},
	note = {Type: Conference paper},
	keywords = {Engineering education, Students, Authentication, Communication skills, E-learning, Engineering authenticity and integrity, Engineering course, Integrity requirements, Network security, Online query \& defense, Online systems, Pedagogical approach, Project based learning, Project implementation, Project management, Student-centred, Teamwork skills},
	pages = {7153 -- 7162},
	annote = {Cited by: 0; Conference name: 33rd Congress of the International Council of the Aeronautical Sciences, ICAS 2022; Conference date: 4 September 2022 through 9 September 2022; Conference code: 188335},
}

@inproceedings{sorensen_information-theoretic_2022,
	title = {An {Information}-theoretic {Approach} to {Prompt} {Engineering} {Without} {Ground} {Truth} {Labels}},
	volume = {1},
	isbn = {978-1-955917-21-6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140494887&partnerID=40&md5=a4deb7c523628fed4c12896cd440c32a},
	abstract = {Pre-trained language models derive substantial linguistic and factual knowledge from the massive corpora on which they are trained, and prompt engineering seeks to align these models to specific tasks. Unfortunately, existing prompt engineering methods require significant amounts of labeled data, access to model parameters, or both. We introduce a new method for selecting prompt templates without labeled examples and without direct access to the model. Specifically, over a set of candidate templates, we choose the template that maximizes the mutual information between the input and the corresponding model output. Across 8 datasets representing 7 distinct NLP tasks, we show that when a template has high mutual information, it also has high accuracy on the task. On the largest model, selecting prompts with our method gets 90\% of the way from the average prompt accuracy to the best prompt accuracy and requires no ground truth labels. © 2022 Association for Computational Linguistics.},
	language = {English},
	booktitle = {Proceedings of the {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics (ACL)},
	author = {Sorensen, Taylor and Robinson, Joshua and Rytting, Christopher Michael and Shaw, Alexander and Rogers, Kyle and Delorey, Alexia and Khalil, Mahmoud and Fulda, Nancy and Wingate, David},
	editor = {S, Muresan and P, Nakov and A, Villavicencio},
	year = {2022},
	note = {ISSN: 0736587X
Type: Conference paper},
	keywords = {Language model, Computational linguistics, Data access, Engineering methods, Factual knowledge, Ground truth, Information theory, Information-theoretic approach, Labeled data, Linguistic knowledge, Mutual informations, Specific tasks},
	pages = {819 -- 862},
	annote = {Cited by: 10; Conference name: 60th Annual Meeting of the Association for Computational Linguistics, ACL 2022; Conference date: 22 May 2022 through 27 May 2022; Conference code: 181737},
}

@article{short_artificially_2023,
	title = {The artificially intelligent entrepreneur: {ChatGPT}, prompt engineering, and entrepreneurial rhetoric creation},
	volume = {19},
	issn = {23526734},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151268343&doi=10.1016%2fj.jbvi.2023.e00388&partnerID=40&md5=305892033a2adab728d16b0cb27167ec},
	doi = {10.1016/j.jbvi.2023.e00388},
	abstract = {To better understand the role of artificial intelligence in the development of entrepreneurial rhetoric, we examine how generative language models such as ChatGPT serve as viable tools for content creation. Using an established framework for examining CEO celebrity (Creator, Transformer, Rebel, and Savior), we illustrate how such models can effectively produce and refine elevator pitches, social media pitches, and crowdfunding pitches commonly used in the study of entrepreneurial rhetoric. We demonstrate ChatGPT's ability to mimic each celebrity CEO archetype by prompting language in the style of exemplars, including Elon Musk, Indra Nooyi, Tony Hsieh, and Lisa Su. Implications of prompt engineering—the fine-tuning of inputs fed into language models to produce precise output—for entrepreneurship research and practice are discussed. We conclude by advancing the idea that the emergent and enduring value of generative models is, at its core, dependent on effective prompt engineering. © 2023 Elsevier Inc.},
	language = {English},
	journal = {Journal of Business Venturing Insights},
	author = {Short, Cole E. and Short, Jeremy C.},
	year = {2023},
	note = {Publisher: Elsevier Inc.
Type: Article},
	annote = {Cited by: 13},
}

@article{korzynski_artificial_2023,
	title = {Artificial intelligence prompt engineering as a new digital competence: {Analysis} of generative {AI} technologies such as {ChatGPT}},
	volume = {11},
	issn = {2353883X},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175052409&doi=10.15678%2fEBER.2023.110302&partnerID=40&md5=8de874dbb66448a57ec1c9a540e46b25},
	doi = {10.15678/EBER.2023.110302},
	abstract = {Objective: The article aims to offer a thorough examination and comprehension of the challenges and pro-spects connected with artificial intelligence (AI) prompt engineering. Our research aimed to create a theoretical framework that would highlight optimal approaches in the field of AI prompt engineering. Research Design \& Methods: This research utilized a narrative and critical literature review and established a conceptual framework derived from existing literature taking into account both academic and practitioner sources. This article should be regarded as a conceptual work that emphasizes the best practices in the domain of AI prompt engineering. Findings: Based on the conducted deep and extensive query of academic and practitioner literature on the subject, as well as professional press and Internet portals, we identified various insights for effective AI prompt engineering. We provide specific prompting strategies. Implications \& Recommendations: The study revealed the profound implications of AI prompt engineering across various domains such as entrepreneurship, art, science, and healthcare. We demonstrated how the effective crafting of prompts can significantly enhance the performance of large language models (LLMs), gen-erating more accurate and contextually relevant results. Our findings offer valuable insights for AI practition-ers, researchers, educators, and organizations integrating AI into their operations, emphasizing the need to invest time and resources in prompt engineering. Moreover, we contributed the AI PROMPT framework to the field, providing clear and actionable guidelines for text-to-text prompt engineering. Contribution \& Value Added: The value of this study lies in its comprehensive exploration of AI prompt engineering as a digital competence. By building upon existing research and prior literature, this study aimed to provide a deeper understanding of the intricacies involved in AI prompt engineering and its role as a digital competence. © 2023, Cracow University of Economics. All rights reserved.},
	language = {English},
	number = {3},
	journal = {Entrepreneurial Business and Economics Review},
	author = {Korzynski, Pawel and Mazurek, Grzegorz and Krzypkowska, Pamela and Kurasinski, Artur},
	year = {2023},
	note = {Publisher: Cracow University of Economics
Type: Article},
	pages = {25 -- 37},
	annote = {Cited by: 1; All Open Access, Gold Open Access},
}

@article{wu_simpletext_2023,
	title = {{SimpleText} {Best} of {Labs} in {CLEF}-2022: {Simplify} {Text} {Generation} with {Prompt} {Engineering}},
	volume = {14163 LNCS},
	issn = {03029743},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172296254&doi=10.1007%2f978-3-031-42448-9_17&partnerID=40&md5=4b791715e532fa35ac4ffacb60f81c95},
	doi = {10.1007/978-3-031-42448-9_17},
	abstract = {This paper reports our approach to the SimpleText@CLEF-2022. For the task 1: what is in (or out)?, we designed a two-stage filtering scheme that utilizes the traditional keyword finding approach TF-IDF score to find the important documents in the first stage and the important sentences in the second stage. The result is comparable to manual run and ranked first in task 1. For the Task 3: Rewrite this!, our system adopts the T5 generation model to rewrite the original sentences. We fine-tuned the model to generate simplified sentence. The result ranked second in task 3. The simplified sentence generated by T5 model cannot fully express the meaning of the original sentence, in a following further experiments, we adopted the GPT3.5 and GPT4 models to generate simple text, and they give better results according to in our evaluation metrics based on readability and vocabulary simplicity. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	language = {English},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Wu, Shih-Hung and Huang, Hong-Yi},
	editor = {A, Arampatzis and E, Kanoulas and M, Aliannejadi and T, Tsikrika and S, Vrochidis and A, Giachanou and D, Li and M, Vlachos and G, Faggioli and N, Ferro},
	year = {2023},
	note = {ISBN: 978-303142447-2
Publisher: Springer Science and Business Media Deutschland GmbH
Type: Conference paper},
	keywords = {Evaluation metrics, Filtering schemes, Gpt3.5 model, GPT4 model, Information retrieval systems, Simple text generation, Simple++, T5 model, Text generations, TF-IDF},
	pages = {198 -- 208},
	annote = {Cited by: 0; Conference name: Proceedings of the 14th International Conference of the Cross-Language Evaluation Forum for European Languages, CLEF 2023; Conference date: 18 September 2023 through 21 September 2023; Conference code: 300519},
}

@article{lo_art_2023,
	title = {The {Art} and {Science} of {Prompt} {Engineering}: {A} {New} {Literacy} in the {Information} {Age}},
	issn = {10875301},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163183360&doi=10.1080%2f10875301.2023.2227621&partnerID=40&md5=44a2b871f27a2b3ed8bc8cceafd798e9},
	doi = {10.1080/10875301.2023.2227621},
	abstract = {The novel discipline of prompt-engineering is a combination of artificial intelligence, linguistics, and user experience design. Crafting effective prompts for AI models like OpenAI’s GPT can optimize the quality of generated output, especially in reference services. The article explains the CLEAR Framework—a guiding tool that incorporates principles of conciseness, logic, explicitness, adaptability, and reflectiveness in prompt crafting. Despite challenges including biases, ethical concerns, and keeping up with rapidly evolving AI capabilities, the field also presents opportunities for growth. The article concludes with a call to action for library professionals to embrace and master prompt-engineering as an essential skill. © 2023 The Author(s). Published with license by Taylor \& Francis Group, LLC.},
	language = {English},
	journal = {Internet Reference Services Quarterly},
	author = {Lo, Leo S.},
	year = {2023},
	note = {Publisher: Routledge
Type: Note},
	keywords = {artificial intelligence, linguistics, literacy, logic, note},
	annote = {Cited by: 1},
}

@inproceedings{yu_exploring_2023,
	title = {Exploring the {Effectiveness} of {Prompt} {Engineering} for {Legal} {Reasoning} {Tasks}},
	isbn = {978-1-959429-62-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170636120&partnerID=40&md5=78ce35d8498e066f10614020c7210abc},
	abstract = {The use of large language models (LLMs) for zero- or few-shot prompting in natural language processing has given rise to a new research area known as prompt engineering, which shows promising improvement in tasks such as arithmetic and common-sense reasoning. This paper explores the use of such approaches in legal reasoning tasks by conducting experiments on the COLIEE entailment task, which is based on the Japanese Bar exam. We further evaluate zero-shot/few-shot and fine-tuning approaches with and without explanations, alongside various prompting strategies. Our results indicate that while these techniques can improve general performance, the best results are achieved with prompts derived from specific legal reasoning techniques, such as IRAC (Issue, Rule, Application, Conclusion). In addition, we observe that few-shot learning with demonstrations derived from clustering past training data consistently yields high performance on the most recent COLIEE entailment tasks. Through our experiments, we improve the previous best result on the 2021 COLIEE task from 0.7037 to 0.8025 and surpass the best system from 2022 with an accuracy of 0.789. © 2023 Association for Computational Linguistics.},
	language = {English},
	booktitle = {Proceedings of the {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics (ACL)},
	author = {Yu, Fangyi and Quartey, Lee and Schilder, Frank},
	year = {2023},
	note = {ISSN: 0736587X
Type: Conference paper},
	keywords = {Language model, Performance, Zero-shot learning, Computational linguistics, Natural language processing systems, Fine tuning, Language processing, Natural languages, Commonsense reasoning, Legal reasoning, Reasoning tasks, Reasoning techniques, Research areas},
	pages = {13582 -- 13596},
	annote = {Cited by: 0; Conference name: 61st Annual Meeting of the Association for Computational Linguistics, ACL 2023; Conference date: 9 July 2023 through 14 July 2023; Conference code: 192867},
}

@article{lim_artificial_2023,
	title = {Artificial intelligence for health message generation: an empirical study using a large language model ({LLM}) and prompt engineering},
	volume = {8},
	issn = {2297900X},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161405789&doi=10.3389%2ffcomm.2023.1129082&partnerID=40&md5=479314891447ac621e0486329631ece1},
	doi = {10.3389/fcomm.2023.1129082},
	abstract = {Introduction: This study introduces and examines the potential of an AI system to generate health awareness messages. The topic of folic acid, a vitamin that is critical during pregnancy, served as a test case. Method: We used prompt engineering to generate awareness messages about folic acid and compared them to the most retweeted human-generated messages via human evaluation with an university sample and another sample comprising of young adult women. We also conducted computational text analysis to examine the similarities between the AI-generated messages and human generated tweets in terms of content and semantic structure. Results: The results showed that AI-generated messages ranked higher in message quality and clarity across both samples. The computational analyses revealed that the AI generated messages were on par with human-generated ones in terms of sentiment, reading ease, and semantic content. Discussion: Overall, these results demonstrate the potential of large language models for message generation. Theoretical, practical, and ethical implications are discussed. Copyright © 2023 Lim and Schmälzle.},
	language = {English},
	journal = {Frontiers in Communication},
	author = {Lim, Sue and Schmälzle, Ralf},
	year = {2023},
	note = {Publisher: Frontiers Media S.A.
Type: Article},
	annote = {Cited by: 3; All Open Access, Gold Open Access},
}

@article{lee_few-shot_2023,
	title = {Few-shot is enough: exploring {ChatGPT} prompt engineering method for automatic question generation in english education},
	issn = {13602357},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175343048&doi=10.1007%2fs10639-023-12249-8&partnerID=40&md5=9e50ac86005b9e7fa8b9b1b63d729729},
	doi = {10.1007/s10639-023-12249-8},
	abstract = {Through design and development research (DDR), we aimed to create a validated automatic question generation (AQG) system using large language models (LLMs) like ChatGPT, enhanced by prompting engineering techniques. While AQG has become increasingly integral to online learning for its efficiency in generating questions, issues such as inconsistent question quality and the absence of transparent and validated evaluation methods persist. Our research focused on creating a prompt engineering protocol tailored for AQG. This protocol underwent several iterations of refinement and validation to improve its performance. By gathering validation scores and qualitative feedback on the produced questions and the system’s framework, we examined the effectiveness of the system. The study findings indicate that our combined use of LLMs and prompt engineering in AQG produces questions with statistically significant validity. Our research further illuminates academic and design considerations for AQG design in English education: (a) certain question types might not be optimal for generation via ChatGPT, (b) ChatGPT sheds light on the potential for collaborative AI-teacher efforts in question generation, especially within English education. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	language = {English},
	journal = {Education and Information Technologies},
	author = {Lee, Unggi and Jung, Haewon and Jeon, Younghoon and Sohn, Younghoon and Hwang, Wonhee and Moon, Jewoong and Kim, Hyeoncheol},
	year = {2023},
	note = {Publisher: Springer
Type: Article},
	annote = {Cited by: 0},
}

@article{kan_hatakeyama-sato_prompt_2023,
	title = {Prompt engineering of {GPT}-4 for chemical research: what can/cannot be done?},
	volume = {3},
	url = {https://doi.org/10.1080/27660400.2023.2260300},
	doi = {10.1080/27660400.2023.2260300},
	number = {1},
	journal = {Science and Technology of Advanced Materials: Methods},
	author = {Kan Hatakeyama-Sato, Naoki Yamane, Yasuhiko Igarashi, Yuta Nabae and Hayakawa, Teruaki},
	year = {2023},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/27660400.2023.2260300},
	pages = {2260300},
}
